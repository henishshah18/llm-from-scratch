{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Softmax and Cross-Entropy Loss\n",
    "\n",
    "Softmax and cross-entropy loss are at the heart of training LLMs and Transformers. Softmax turns logits into probabilities, and cross-entropy measures how well the model predicts the correct class/token.\n",
    "\n",
    "In this notebook, you'll:\n",
    "- Implement softmax from scratch\n",
    "- Explore numerical stability in softmax\n",
    "- Implement cross-entropy loss\n",
    "- See how these are used in LLM training and language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ What is Softmax?\n",
    "\n",
    "Softmax converts a vector of raw scores (logits) into probabilities. In LLMs, the output logits for each token position are passed through softmax to get a probability distribution over the vocabulary.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "### Task:\n",
    "- Write a function to compute softmax for a 1D numpy array of logits.\n",
    "- Add a docstring explaining why softmax is used in LLMs.\n",
    "- Do NOT use any library softmax (no np.exp directly in the return statement‚Äîwrite out the steps).\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Softmax is used in the output layer of every transformer to turn logits into token probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute the softmax of a 1D numpy array of logits.\n",
    "    In LLMs, softmax is used to convert model outputs (logits) into probabilities over the vocabulary.\n",
    "    Args:\n",
    "        logits (np.ndarray): 1D array of raw scores.\n",
    "    Returns:\n",
    "        np.ndarray: Probabilities summing to 1.\n",
    "    \"\"\"\n",
    "    # TODO: Implement softmax step by step (no np.exp in return statement)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Numerical Stability in Softmax\n",
    "\n",
    "Large logits can cause overflow in $e^{z_i}$. To prevent this, we subtract the max logit before exponentiating.\n",
    "\n",
    "### Task:\n",
    "- Modify your softmax function to be numerically stable.\n",
    "- Add a comment explaining why this is important for LLMs (think: large vocabularies, large logits).\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Transformers often output very large logits, so numerical stability is critical for correct probability computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(logits):\n",
    "    \"\"\"\n",
    "    Compute the numerically stable softmax of a 1D numpy array of logits.\n",
    "    Subtracts the max logit for stability (important for LLMs with large vocabularies).\n",
    "    Args:\n",
    "        logits (np.ndarray): 1D array of raw scores.\n",
    "    Returns:\n",
    "        np.ndarray: Probabilities summing to 1.\n",
    "    \"\"\"\n",
    "    # TODO: Implement numerically stable softmax\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss measures how well the predicted probability distribution matches the true distribution. In LLMs, this is used to train the model to predict the next token.\n",
    "\n",
    "**Formula (for one-hot targets):**\n",
    "$$L = -\\sum_{i} y_i \\log(p_i)$$\n",
    "\n",
    "### Task:\n",
    "- Write a function to compute cross-entropy loss given predicted probabilities and a true class index.\n",
    "- Add a docstring explaining why cross-entropy is used in LLMs.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Cross-entropy is the standard loss for language modeling and next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, target_index):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for a single prediction.\n",
    "    In LLMs, this is used to measure how well the model predicts the correct next token.\n",
    "    Args:\n",
    "        probs (np.ndarray): 1D array of predicted probabilities (output of softmax).\n",
    "        target_index (int): Index of the true class/token.\n",
    "    Returns:\n",
    "        float: Cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    # TODO: Implement cross-entropy loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac3490",
   "metadata": {},
   "source": [
    "## üîÅ Gradient of Softmax + Cross-Entropy (Backprop in LLMs)\n",
    "\n",
    "When training LLMs, the combination of softmax and cross-entropy loss has a beautiful property: the gradient with respect to the logits simplifies to:\n",
    "\n",
    "$$ \\nabla L = \\hat{y} - y $$\n",
    "\n",
    "- $\\hat{y}$: predicted probability distribution (output of softmax)\n",
    "- $y$: one-hot encoded true label\n",
    "\n",
    "**Why is this important?**\n",
    "- This formula is used in every transformer and LLM during backpropagation, making the output layer gradient computation efficient and numerically stable.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute the gradient of the softmax + cross-entropy loss with respect to the logits, given the logits and the true class index.\n",
    "- Add a docstring explaining its role in LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_grad(logits, target_index):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the softmax + cross-entropy loss with respect to the logits.\n",
    "    In LLMs, this is used during backpropagation to efficiently compute gradients for the output layer.\n",
    "    Args:\n",
    "        logits (np.ndarray): 1D array of raw model outputs (logits).\n",
    "        target_index (int): Index of the true class/token.\n",
    "    Returns:\n",
    "        np.ndarray: Gradient vector (same shape as logits).\n",
    "    \"\"\"\n",
    "    # TODO: Implement the gradient: grad = softmax(logits) - one_hot(target_index)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Softmax + Cross-Entropy in Language Modeling\n",
    "\n",
    "In LLMs, the model outputs a vector of logits for each token position. These are passed through softmax to get probabilities, and cross-entropy loss is computed against the true next token.\n",
    "\n",
    "### Task:\n",
    "- Given a batch of logits (2D array: batch_size x vocab_size) and true target indices, outline how you would compute the average cross-entropy loss for the batch.\n",
    "- Add comments explaining each step and its relevance to LLM training.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- This is the core of the training loop for every transformer-based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cross_entropy_loss(logits_batch, target_indices):\n",
    "    \"\"\"\n",
    "    Compute the average cross-entropy loss for a batch of logits and target indices.\n",
    "    In LLMs, this is used to train on multiple sequences/tokens at once.\n",
    "    Args:\n",
    "        logits_batch (np.ndarray): 2D array (batch_size x vocab_size) of logits.\n",
    "        target_indices (np.ndarray): 1D array of true token indices (batch_size,).\n",
    "    Returns:\n",
    "        float: Average cross-entropy loss over the batch.\n",
    "    \"\"\"\n",
    "    # TODO: For each example in the batch:\n",
    "    #   1. Compute softmax (numerically stable) on logits\n",
    "    #   2. Compute cross-entropy loss for the true target\n",
    "    #   3. Average the losses\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Why Softmax and Cross-Entropy Matter for LLMs\n",
    "\n",
    "- **Softmax** turns model outputs into probabilities over the vocabulary, enabling sampling and evaluation.\n",
    "- **Cross-entropy loss** is the objective that drives learning in language models‚Äîminimizing it means better next-token prediction.\n",
    "- Every LLM and transformer uses these operations in its output and training loop.\n",
    "\n",
    "In the next notebook, you'll use these concepts to build and train a simple feedforward neural network for classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
