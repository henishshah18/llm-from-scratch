{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Vector and Matrix Basics\n",
    "\n",
    "This notebook introduces the basic operations on vectors and matrices using **NumPy**. These operations are foundational for understanding computations in neural networks and large language models (LLMs).\n",
    "\n",
    "You'll find **tasks** throughout‚Äîtry to implement each one before checking the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Vector and Matrix Initialization\n",
    "\n",
    "Create 1D vectors and 2D matrices using NumPy. Understanding how to define and inspect them is the first step.\n",
    "\n",
    "### Task:\n",
    "- Create a 3D vector `v = [1, 2, 3]`\n",
    "- Create a 2x3 matrix `M = [[1, 2, 3], [4, 5, 6]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Define a 3D vector\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "# TODO: Define a 2x3 matrix\n",
    "M = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print('Vector v:', v)\n",
    "print('Matrix M:\\n', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Dot Product and Cosine Similarity\n",
    "\n",
    "**Dot product** is used in attention mechanisms (`Q¬∑K·µÄ`), similarity checks, and projection.\n",
    "\n",
    "### Task:\n",
    "- Implement dot product of two vectors\n",
    "- Compute cosine similarity\n",
    "\n",
    "Recall: $\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectors\n",
    "A = np.array([1, 2])\n",
    "B = np.array([3, 4])\n",
    "\n",
    "# TODO: Implement dot product\n",
    "dot = np.dot(A, B)\n",
    "\n",
    "# TODO: Compute cosine similarity\n",
    "cos_sim = dot / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "print('Dot product:', dot)\n",
    "print('Cosine similarity:', cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Matrix Multiplication\n",
    "\n",
    "Used everywhere: feedforward layers (`W¬∑x`), attention scores, transformer blocks.\n",
    "\n",
    "### Task:\n",
    "- Multiply a 2x3 matrix with a 3x1 vector.\n",
    "- Explain the shape flow: (2x3) ¬∑ (3x1) ‚Üí (2x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix and vector\n",
    "mat = np.array([[1, 2, 3], [4, 5, 6]])  # shape: (2, 3)\n",
    "vec = np.array([[1], [0], [-1]])       # shape: (3, 1)\n",
    "\n",
    "# TODO: Matrix-vector multiplication\n",
    "output = np.dot(mat, vec)\n",
    "\n",
    "print('Output shape:', output.shape)\n",
    "print('Output:\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Element-wise Operations & Broadcasting\n",
    "\n",
    "Used in activation functions, residual connections, etc.\n",
    "\n",
    "### Task:\n",
    "- Add a vector to each row of a matrix (broadcasting).\n",
    "- Square all elements in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "vector = np.array([1, 0, -1])\n",
    "\n",
    "# TODO: Broadcasted addition\n",
    "broadcasted_sum = matrix + vector\n",
    "\n",
    "# TODO: Element-wise square\n",
    "squared = matrix ** 2\n",
    "\n",
    "print('Broadcasted sum:\\n', broadcasted_sum)\n",
    "print('Squared matrix:\\n', squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Vector Norms and Unit Vectors\n",
    "\n",
    "Norms are used in normalization (LayerNorm), cosine similarity, etc.\n",
    "\n",
    "### Task:\n",
    "- Compute L2 norm of a vector.\n",
    "- Normalize it to a unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.array([3, 4])\n",
    "\n",
    "# TODO: Compute L2 norm\n",
    "norm = np.linalg.norm(vec)\n",
    "\n",
    "# TODO: Normalize the vector\n",
    "unit_vector = vec / norm\n",
    "\n",
    "print('L2 norm:', norm)\n",
    "print('Unit vector:', unit_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Matrix Transpose\n",
    "\n",
    "The **transpose** of a matrix flips its rows and columns. Used in backpropagation and attention.\n",
    "\n",
    "### Task:\n",
    "- Transpose a 2x3 matrix to a 3x2 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# TODO: Transpose the matrix\n",
    "M_T = M.T\n",
    "\n",
    "print('Transposed matrix:\\n', M_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü∞ Check for Square Matrix\n",
    "\n",
    "A square matrix has the same number of rows and columns. This is important for certain operations (e.g., self-attention weight matrices).\n",
    "\n",
    "### Task:\n",
    "- Write a function to check if a matrix is square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_square_matrix(M):\n",
    "    return M.shape[0] == M.shape[1]\n",
    "\n",
    "# Test\n",
    "print(is_square_matrix(np.array([[1,2],[3,4]])))  # True\n",
    "print(is_square_matrix(np.array([[1,2,3],[4,5,6]])))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Summary: Why This Matters for LLMs\n",
    "\n",
    "These operations form the low-level math behind:\n",
    "- Attention mechanism (`Q¬∑K·µÄ`, softmax)\n",
    "- Feedforward layers (`Wx + b`)\n",
    "- Token embeddings and projection layers\n",
    "- LayerNorm and residual connections\n",
    "\n",
    "Make sure you're comfortable with these ‚Äî they'll show up in every component of the LLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Why These Basics Matter for LLMs\n",
    "\n",
    "Congratulations! You've practiced the core vector and matrix operations that are the mathematical backbone of neural networks and large language models (LLMs).\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **Vector and matrix initialization**: All data, weights, and activations in LLMs are represented as vectors or matrices.\n",
    "- **Dot product & cosine similarity**: Used in attention mechanisms to measure similarity between queries and keys.\n",
    "- **Matrix multiplication**: Fundamental for feedforward layers, attention score computation, and transforming embeddings.\n",
    "- **Element-wise operations & broadcasting**: Power activation functions, residual connections, and normalization steps.\n",
    "- **Norms and normalization**: Essential for techniques like LayerNorm and for stable training.\n",
    "- **Transpose and shape checks**: Required for aligning data and weights in multi-head attention and backpropagation.\n",
    "\n",
    "**Next Steps:**\n",
    "- These operations will appear in every component of an LLM, from token embedding to attention to output projection.\n",
    "- As you move forward, try to visualize the data flow and shapes at each step‚Äîthis will help you debug and design your own models.\n",
    "\n",
    "Ready? In the next notebook, you'll dive into derivatives and the chain rule, which are essential for training neural networks from scratch!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
