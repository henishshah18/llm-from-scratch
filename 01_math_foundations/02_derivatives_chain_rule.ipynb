{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926c337f",
   "metadata": {},
   "source": [
    "# 02 - Derivatives and the Chain Rule\n",
    "\n",
    "Understanding derivatives and the chain rule is essential for training neural networks, including LLMs. Every weight update in an LLM is powered by these concepts!\n",
    "\n",
    "In this notebook, you'll:\n",
    "- Compute derivatives of basic functions\n",
    "- Implement and differentiate activation functions\n",
    "- Apply the chain rule (the backbone of backpropagation)\n",
    "- See how gradients flow through a mini neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad85b7",
   "metadata": {},
   "source": [
    "## ðŸ§® Scalar Derivatives\n",
    "\n",
    "Derivatives tell us how a function changes as its input changes. In neural networks, this tells us how to adjust weights to reduce loss.\n",
    "\n",
    "### Task:\n",
    "- Implement the derivative of a simple scalar function $f(x) = x^2 + 3x + 2$.\n",
    "- Verify numerically using finite difference.\n",
    "\n",
    "**LLM/NN Context:**\n",
    "- Every parameter update in an LLM is based on derivatives of the loss with respect to that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c10b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "# Analytical derivative\n",
    "def df_dx(x):\n",
    "    return 2*x + 3\n",
    "\n",
    "# Numerical derivative (finite difference)\n",
    "def numerical_derivative(f, x, eps=1e-5):\n",
    "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "x = 5.0\n",
    "print('Analytical:', df_dx(x))\n",
    "print('Numerical:', numerical_derivative(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ff672",
   "metadata": {},
   "source": [
    "## ðŸ” Derivatives of Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity in neural networks. Their derivatives are needed for backpropagation.\n",
    "\n",
    "### Task:\n",
    "- Implement the sigmoid function and its derivative\n",
    "- Implement ReLU and its derivative\n",
    "\n",
    "**LLM/NN Context:**\n",
    "- Transformers use activation functions (like GELU, ReLU) in every feedforward block. Their derivatives are used in every backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "x_vals = np.array([-2.0, 0.0, 2.0])\n",
    "print('Sigmoid derivative:', sigmoid_derivative(x_vals))\n",
    "print('ReLU derivative:', relu_derivative(x_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d70d16",
   "metadata": {},
   "source": [
    "## ðŸ”— Chain Rule\n",
    "\n",
    "The chain rule lets us compute derivatives of composite functions. This is the backbone of backpropagation in neural networks and LLMs.\n",
    "\n",
    "### Example:\n",
    "If $f(x) = g(h(x))$, then $\\frac{df}{dx} = \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$\n",
    "\n",
    "### Task:\n",
    "- Let $h(x) = x^2$, $g(h) = 3h + 1$\n",
    "- Write $f(x) = g(h(x))$, then compute $df/dx$ using the chain rule.\n",
    "\n",
    "**LLM/NN Context:**\n",
    "- Every layer in an LLM is a function of the previous layer. The chain rule lets us propagate gradients backward through all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return x**2\n",
    "\n",
    "def g(h_val):\n",
    "    return 3*h_val + 1\n",
    "\n",
    "def f(x):\n",
    "    return g(h(x))\n",
    "\n",
    "def df_dx(x):\n",
    "    dh_dx = 2*x\n",
    "    dg_dh = 3\n",
    "    return dg_dh * dh_dx\n",
    "\n",
    "x = 4.0\n",
    "print('f(x):', f(x))\n",
    "print('df/dx (chain rule):', df_dx(x))\n",
    "print('df/dx (numerical):', numerical_derivative(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef4655",
   "metadata": {},
   "source": [
    "## ðŸ§  Chain Rule in a Mini Neural Network\n",
    "\n",
    "Letâ€™s apply this to a simple feedforward computation:\n",
    "- $z = Wx + b$\n",
    "- $a = \\text{ReLU}(z)$\n",
    "- $y = W_a a + b_a$\n",
    "\n",
    "### Task:\n",
    "- Do a forward pass with sample data.\n",
    "- Manually compute all gradients using the chain rule.\n",
    "\n",
    "**LLM/NN Context:**\n",
    "- This is the core of the backward pass in every transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample values\n",
    "x = np.array([[1.0], [2.0]])      # input (2x1)\n",
    "W1 = np.array([[1.0, -1.0]])      # weights (1x2)\n",
    "b1 = np.array([[0.5]])            # bias (1x1)\n",
    "\n",
    "# Forward pass\n",
    "z1 = np.dot(W1, x) + b1           # (1x1)\n",
    "a1 = relu(z1)                     # (1x1)\n",
    "\n",
    "W2 = np.array([[2.0]])            # weights (1x1)\n",
    "b2 = np.array([[1.0]])            # bias (1x1)\n",
    "y = np.dot(W2, a1) + b2           # (1x1)\n",
    "\n",
    "# Backward pass (manual gradients)\n",
    "dy = 1                            # âˆ‚L/âˆ‚y, assume loss gradient = 1\n",
    "dW2 = dy * a1                     # âˆ‚L/âˆ‚W2\n",
    "db2 = dy                          # âˆ‚L/âˆ‚b2\n",
    "da1 = dy * W2                     # âˆ‚L/âˆ‚a1\n",
    "dz1 = da1 * relu_derivative(z1)   # âˆ‚L/âˆ‚z1\n",
    "dW1 = np.dot(dz1, x.T)            # âˆ‚L/âˆ‚W1\n",
    "db1 = dz1                         # âˆ‚L/âˆ‚b1\n",
    "\n",
    "print('dW2:', dW2)\n",
    "print('db2:', db2)\n",
    "print('dW1:', dW1)\n",
    "print('db1:', db1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec583c89",
   "metadata": {},
   "source": [
    "## ðŸ§  Final Summary: Why Derivatives and the Chain Rule Matter for LLMs\n",
    "\n",
    "- Derivatives and the chain rule are the foundation of backpropagation, which powers learning in all neural networksâ€”including LLMs.\n",
    "- Every parameter update in an LLM is based on gradients computed using these rules.\n",
    "- Understanding how gradients flow through each layer will help you debug and design your own models.\n",
    "\n",
    "**Next:** In the next notebook, you'll use these gradients to implement and train a simple neural network from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
