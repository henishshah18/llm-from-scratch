# LLM From Scratch

This repository is a step-by-step guide to understanding and building foundational components of Large Language Models (LLMs). It is organized into multiple sections, each focusing on a specific topic, with Jupyter notebooks to provide hands-on learning.

## Sections Overview

### 01. Math Foundations
- **01_vector_matrix_basics.ipynb**: Introduction to vectors and matrices.
- **02_derivatives_chain_rule.ipynb**: Understanding derivatives and the chain rule.
- **03_softmax_cross_entropy.ipynb**: Softmax function and cross-entropy loss.

### 02. Feedforward Neural Networks
- **04_feedforward_nn.ipynb**: Building a simple feedforward neural network.
- **05_backprop_manual.ipynb**: Manual implementation of backpropagation.
- **06_xor_classification.ipynb**: Solving the XOR problem using a neural network.

### 03. Optimization
- **07_gradient_descent.ipynb**: Gradient descent optimization.
- **08_optimizer_comparisons.ipynb**: Comparing different optimization algorithms.

### 04. Recurrent Neural Networks (RNNs)
- **09_rnn_from_scratch.ipynb**: Implementing RNNs from scratch.
- **10_rnn_backprop_through_time.ipynb**: Backpropagation through time for RNNs.
- **11_rnn_char_language_model.ipynb**: Character-level language modeling with RNNs.

### 05. LSTM and GRU
- **12_lstm_from_scratch.ipynb**: Building LSTMs from scratch.
- **13_gru_from_scratch.ipynb**: Building GRUs from scratch.
- **14_rnn_lstm_gru_comparison.ipynb**: Comparing RNNs, LSTMs, and GRUs.

### 06. Encoder-Decoder Architectures
- **15_encoder_decoder_no_attention.ipynb**: Encoder-decoder without attention.
- **16_add_attention_mechanism.ipynb**: Adding attention mechanisms.
- **17_positional_encoding.ipynb**: Understanding positional encoding.
- **18_teacher_forcing.ipynb**: Teacher forcing in sequence models.

### 07. Transformers
- **19_self_attention_from_scratch.ipynb**: Self-attention mechanism.
- **20_multihead_attention.ipynb**: Multi-head attention.
- **21_positional_encoding_transformer.ipynb**: Positional encoding in transformers.
- **22_transformer_block.ipynb**: Transformer block implementation.
- **23_mini_transformer_language_model.ipynb**: Mini transformer-based language model.

### 08. Tokenization and Inference
- **24_bpe_tokenizer.ipynb**: Byte Pair Encoding (BPE) tokenizer.
- **25_sampling_strategies.ipynb**: Sampling strategies for text generation.
- **26_tokenize_embed_predict_decode.ipynb**: Tokenization, embedding, prediction, and decoding.
- **27_topk_vs_topp.ipynb**: Top-k vs. Top-p sampling.

### 09. Advanced Topics
- **28_rlhf_overview.ipynb**: Overview of Reinforcement Learning with Human Feedback (RLHF).
- **29_rag_schematic.ipynb**: Retrieval-Augmented Generation (RAG) schematic.
- **30_pretraining_vs_finetuning.ipynb**: Pretraining vs. fine-tuning.
- **31_prompting_vs_finetuning_vs_rag.ipynb**: Comparing prompting, fine-tuning, and RAG.
- **32_model_compression.ipynb**: Techniques for model compression.

## How to Use
1. Clone the repository.
2. Navigate to the desired section.
3. Open the Jupyter notebooks to explore the concepts and code.

## Requirements
- Python 3.8+
- Jupyter Notebook
- Required libraries are listed in the notebooks or can be installed as needed.

## License
This project is licensed under the MIT License.