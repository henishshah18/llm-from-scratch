{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c928b1ca",
   "metadata": {},
   "source": [
    "# 06 - XOR Classification with a Neural Network\n",
    "\n",
    "The XOR problem is a classic test for neural networks: it cannot be solved by a single linear layer, so it demonstrates the need for nonlinearity and multiple layers‚Äîjust like in LLMs and transformers.\n",
    "\n",
    "In this notebook, you'll scaffold a neural network to solve XOR, and see how the same principles apply to much larger models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e9996",
   "metadata": {},
   "source": [
    "## üî¢ The XOR Dataset\n",
    "\n",
    "The XOR dataset consists of four points:\n",
    "- Inputs: (0,0), (0,1), (1,0), (1,1)\n",
    "- Labels: 0, 1, 1, 0\n",
    "\n",
    "**LLM/NN Context:**\n",
    "- This is a minimal example of a non-linearly separable problem, showing why deep models (with nonlinearity) are needed.\n",
    "\n",
    "### Task:\n",
    "- Scaffold code to create the XOR dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create XOR input and label arrays (X, y)\n",
    "# X: shape (4, 2), y: shape (4,)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3fef3",
   "metadata": {},
   "source": [
    "## üßÆ Neural Network Architecture\n",
    "\n",
    "To solve XOR, you need at least one hidden layer with a non-linear activation (e.g., ReLU, sigmoid, or GELU).\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- LLMs use deep stacks of such layers to model complex relationships between tokens.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to initialize the weights and biases for a two-layer neural network.\n",
    "- Add a docstring explaining the role of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfdfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xor_network(input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a two-layer neural network.\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int): Number of hidden units.\n",
    "        output_dim (int): Number of output classes.\n",
    "    Returns:\n",
    "        dict: Parameters (W1, b1, W2, b2)\n",
    "    \"\"\"\n",
    "    # TODO: Initialize weights and biases (random or zeros)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf849e6",
   "metadata": {},
   "source": [
    "## üîó Forward Pass\n",
    "\n",
    "The forward pass computes the output of the network for a given input.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- This is the same computation performed in every transformer block, just at a much larger scale.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for the forward pass through the two-layer network (with nonlinearity).\n",
    "- Add a docstring explaining the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_forward(X, params, activation_fn):\n",
    "    \"\"\"\n",
    "    Forward pass for a two-layer neural network on XOR data.\n",
    "    Args:\n",
    "        X (np.ndarray): Input data (batch_size x input_dim)\n",
    "        params (dict): Network parameters (W1, b1, W2, b2)\n",
    "        activation_fn (callable): Nonlinear activation function\n",
    "    Returns:\n",
    "        np.ndarray: Output logits (batch_size x output_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the forward pass (linear -> activation -> linear)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008551a1",
   "metadata": {},
   "source": [
    "## üßÆ Loss Function: Binary Cross-Entropy\n",
    "\n",
    "For binary classification, use the binary cross-entropy loss.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- LLMs use cross-entropy loss for next-token prediction; here, we use it for binary classification.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute binary cross-entropy loss for predictions and true labels.\n",
    "- Add a docstring explaining its role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54539c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Compute binary cross-entropy loss.\n",
    "    Args:\n",
    "        preds (np.ndarray): Predicted probabilities (batch_size,)\n",
    "        targets (np.ndarray): True labels (batch_size,)\n",
    "    Returns:\n",
    "        float: Average loss\n",
    "    \"\"\"\n",
    "    # TODO: Implement binary cross-entropy loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bc476",
   "metadata": {},
   "source": [
    "## üîÅ Training Loop (Gradient Descent)\n",
    "\n",
    "Train the network by updating weights using gradients from backpropagation.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- LLMs are trained using gradient descent on massive datasets; here, you'll do it for XOR.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for the training loop: forward pass, loss, backward pass, parameter update.\n",
    "- Add a docstring explaining each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fa5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_network(X, y, params, activation_fn, loss_fn, lr, epochs):\n",
    "    \"\"\"\n",
    "    Train the XOR neural network using gradient descent.\n",
    "    Args:\n",
    "        X (np.ndarray): Input data.\n",
    "        y (np.ndarray): True labels.\n",
    "        params (dict): Network parameters.\n",
    "        activation_fn (callable): Activation function.\n",
    "        loss_fn (callable): Loss function.\n",
    "        lr (float): Learning rate.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        dict: Trained parameters.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop (forward, loss, backward, update)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c800f",
   "metadata": {},
   "source": [
    "## üìà Evaluation: Accuracy\n",
    "\n",
    "After training, evaluate the model's accuracy on the XOR dataset.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Evaluation metrics (like accuracy, perplexity) are used to measure LLM performance.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute accuracy given predictions and true labels.\n",
    "- Add a docstring explaining its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3843860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, targets):\n",
    "    \"\"\"\n",
    "    Compute accuracy for binary predictions.\n",
    "    Args:\n",
    "        preds (np.ndarray): Predicted probabilities or logits.\n",
    "        targets (np.ndarray): True labels.\n",
    "    Returns:\n",
    "        float: Accuracy (0 to 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement accuracy computation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a775b81",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Why XOR Matters for LLMs\n",
    "\n",
    "- XOR demonstrates the need for nonlinearity and multiple layers‚Äîcore ideas in LLMs and transformers.\n",
    "- Training and evaluating on XOR builds intuition for how deep models learn complex patterns.\n",
    "- The same principles scale up to the massive neural networks used in language models.\n",
    "\n",
    "In the next notebook, you'll explore optimization algorithms that make training deep networks efficient!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
