{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3a653a",
   "metadata": {},
   "source": [
    "# 04 - Feedforward Neural Networks (FFN) in LLMs\n",
    "\n",
    "Feedforward neural networks (FFNs) are the core building blocks of transformers and LLMs. Every transformer block contains a position-wise FFN that processes each token embedding independently after self-attention.\n",
    "\n",
    "In this notebook, you'll scaffold the components of a feedforward neural network, step by step, as they appear in LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba077d",
   "metadata": {},
   "source": [
    "## üî¢ Linear Transformation (Affine Layer)\n",
    "\n",
    "The first step in a feedforward layer is a linear transformation: $y = Wx + b$.\n",
    "\n",
    "**LLM Context:**\n",
    "- In transformers, this is used to project token embeddings to a higher-dimensional space before applying non-linearity.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute the linear transformation for a batch of inputs.\n",
    "- Add a docstring explaining its role in LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be50e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear transformation y = Wx + b for a batch of inputs.\n",
    "    In LLMs, this projects token embeddings or hidden states to a new space.\n",
    "    Args:\n",
    "        X (np.ndarray): Input batch (batch_size x input_dim)\n",
    "        W (np.ndarray): Weight matrix (output_dim x input_dim)\n",
    "        b (np.ndarray): Bias vector (output_dim,)\n",
    "    Returns:\n",
    "        np.ndarray: Output batch (batch_size x output_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the linear transformation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f645d6",
   "metadata": {},
   "source": [
    "## üßÆ Non-Linearity (Activation Function)\n",
    "\n",
    "After the linear transformation, a non-linear activation is applied. Transformers often use GELU or ReLU.\n",
    "\n",
    "**LLM Context:**\n",
    "- Non-linearity allows the model to learn complex patterns beyond what a single linear layer can represent.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for the GELU activation (used in most LLMs).\n",
    "- Add a docstring explaining why GELU is preferred in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c07aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Compute the GELU activation function.\n",
    "    GELU is the default non-linearity in most transformer-based LLMs (e.g., BERT, GPT).\n",
    "    Args:\n",
    "        x (np.ndarray): Input array.\n",
    "    Returns:\n",
    "        np.ndarray: Output after applying GELU.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the GELU activation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0f0f0",
   "metadata": {},
   "source": [
    "## üîÅ Stacking Layers: The Feedforward Block\n",
    "\n",
    "A feedforward block in a transformer consists of two linear layers with a non-linearity in between:\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "**LLM Context:**\n",
    "- This block is applied independently to each token position after self-attention in every transformer layer.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for the full feedforward block (two linear layers + GELU).\n",
    "- Add a docstring explaining its role in the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_block(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Compute the transformer feedforward block: two linear layers with GELU in between.\n",
    "    This is applied position-wise to each token embedding in LLMs.\n",
    "    Args:\n",
    "        x (np.ndarray): Input (batch_size x input_dim)\n",
    "        W1, b1: First layer weights and bias\n",
    "        W2, b2: Second layer weights and bias\n",
    "    Returns:\n",
    "        np.ndarray: Output (batch_size x output_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the feedforward block\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a3c0b",
   "metadata": {},
   "source": [
    "## üß† Residual Connections (Skip Connections)\n",
    "\n",
    "Transformers add the input to the output of the feedforward block (residual connection) before layer normalization.\n",
    "\n",
    "**LLM Context:**\n",
    "- Residual connections help gradients flow and enable training of very deep models like GPT-3.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function that adds a residual connection: output = input + feedforward_output.\n",
    "- Add a docstring explaining why this is critical for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residual(input_tensor, output_tensor):\n",
    "    \"\"\"\n",
    "    Add a residual (skip) connection: output = input + output_tensor.\n",
    "    Residual connections are critical for training deep LLMs and transformers.\n",
    "    Args:\n",
    "        input_tensor (np.ndarray): Original input.\n",
    "        output_tensor (np.ndarray): Output from feedforward block.\n",
    "    Returns:\n",
    "        np.ndarray: Result after adding residual connection.\n",
    "    \"\"\"\n",
    "    # TODO: Add input_tensor and output_tensor\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa355fb2",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Feedforward NNs in LLMs\n",
    "\n",
    "- Feedforward blocks are the main non-attention computation in every transformer layer.\n",
    "- They consist of two linear layers, a non-linearity (usually GELU), and a residual connection.\n",
    "- Mastering these steps is essential for understanding and building LLMs.\n",
    "\n",
    "In the next notebook, you'll learn how to compute gradients and backpropagate through these layers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
