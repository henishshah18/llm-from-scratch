{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c2cd3d",
   "metadata": {},
   "source": [
    "# 05 - Manual Backpropagation in Feedforward Networks\n",
    "\n",
    "Backpropagation is the algorithm that enables LLMs and transformers to learn by updating their weights using gradients. In this notebook, you'll manually compute gradients for a simple feedforward network, mirroring the process that happens in every transformer block during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cd083",
   "metadata": {},
   "source": [
    "## üî¢ Forward Pass Recap\n",
    "\n",
    "Before computing gradients, let's recall the forward pass for a single-layer feedforward network:\n",
    "\n",
    "$$ y = W x + b $$\n",
    "\n",
    "**LLM Context:**\n",
    "- This is the basic computation in every transformer feedforward block.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for the forward pass (linear transformation + activation).\n",
    "- Add a docstring explaining its role in LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W, b, activation_fn):\n",
    "    \"\"\"\n",
    "    Compute the forward pass: y = activation_fn(Wx + b).\n",
    "    In LLMs, this is used in the feedforward block of each transformer layer.\n",
    "    Args:\n",
    "        x (np.ndarray): Input vector.\n",
    "        W (np.ndarray): Weight matrix.\n",
    "        b (np.ndarray): Bias vector.\n",
    "        activation_fn (callable): Activation function (e.g., GELU, ReLU).\n",
    "    Returns:\n",
    "        np.ndarray: Output vector.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the forward pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71ff2f",
   "metadata": {},
   "source": [
    "## üîó Manual Gradient Calculation (Backpropagation)\n",
    "\n",
    "To train a neural network, we need to compute the gradients of the loss with respect to each parameter (weights and biases). This is done using the chain rule.\n",
    "\n",
    "**LLM Context:**\n",
    "- Every parameter in a transformer is updated using gradients computed via backpropagation.\n",
    "\n",
    "### Task:\n",
    "- Scaffold functions to compute the gradients of the loss with respect to W, b, and x for a single-layer network.\n",
    "- Add docstrings explaining the role of each gradient in LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dy, x, W, b, activation_fn, activation_grad_fn):\n",
    "    \"\"\"\n",
    "    Compute gradients for a single-layer feedforward network.\n",
    "    Args:\n",
    "        dy (np.ndarray): Gradient of the loss with respect to the output (dL/dy).\n",
    "        x (np.ndarray): Input vector.\n",
    "        W (np.ndarray): Weight matrix.\n",
    "        b (np.ndarray): Bias vector.\n",
    "        activation_fn (callable): Activation function used in the forward pass.\n",
    "        activation_grad_fn (callable): Derivative of the activation function.\n",
    "    Returns:\n",
    "        dW (np.ndarray): Gradient w.r.t. W.\n",
    "        db (np.ndarray): Gradient w.r.t. b.\n",
    "        dx (np.ndarray): Gradient w.r.t. x (for chaining to previous layers).\n",
    "    \"\"\"\n",
    "    # TODO: Implement manual backpropagation for a single-layer network\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04053713",
   "metadata": {},
   "source": [
    "## üßÆ Backpropagation Through a Two-Layer Feedforward Block\n",
    "\n",
    "Transformers use two linear layers with a non-linearity in between. Let's scaffold the backward pass for this structure:\n",
    "\n",
    "$$ h = \\text{activation}(W_1 x + b_1) $$\n",
    "$$ y = W_2 h + b_2 $$\n",
    "\n",
    "**LLM Context:**\n",
    "- This is the exact structure of the feedforward block in every transformer layer.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute gradients for all parameters in a two-layer feedforward block.\n",
    "- Add a docstring explaining how this mirrors the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c41174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_two_layer(x, W1, b1, W2, b2, activation_fn, activation_grad_fn, dy):\n",
    "    \"\"\"\n",
    "    Compute gradients for a two-layer feedforward block (as in transformers).\n",
    "    Args:\n",
    "        x (np.ndarray): Input vector.\n",
    "        W1, b1: First layer weights and bias.\n",
    "        W2, b2: Second layer weights and bias.\n",
    "        activation_fn (callable): Activation function.\n",
    "        activation_grad_fn (callable): Derivative of activation function.\n",
    "        dy (np.ndarray): Gradient of loss w.r.t. output.\n",
    "    Returns:\n",
    "        dW1, db1, dW2, db2, dx: Gradients for all parameters and input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement manual backpropagation for two-layer feedforward block\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4a623",
   "metadata": {},
   "source": [
    "## üîÅ Gradient Flow and Residual Connections\n",
    "\n",
    "In transformers, the output of the feedforward block is added to the input (residual connection) before layer normalization. It's important to understand how gradients flow through this addition.\n",
    "\n",
    "**LLM Context:**\n",
    "- Residual connections help gradients flow backward, enabling deep LLMs to train effectively.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute the gradient of the loss with respect to the input when a residual connection is used.\n",
    "- Add a docstring explaining why this is important for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f721db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_backward(dout):\n",
    "    \"\"\"\n",
    "    Compute the gradient flow through a residual (skip) connection.\n",
    "    In LLMs, this ensures both the input and the feedforward block receive gradients.\n",
    "    Args:\n",
    "        dout (np.ndarray): Gradient of loss w.r.t. output (after addition).\n",
    "    Returns:\n",
    "        dx_input (np.ndarray): Gradient w.r.t. the original input.\n",
    "        dx_ffn (np.ndarray): Gradient w.r.t. the feedforward output.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient flow through residual connection\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93504c5f",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Manual Backprop in LLMs\n",
    "\n",
    "- Manual backpropagation builds intuition for how gradients flow through each layer and parameter in a transformer.\n",
    "- Every weight in an LLM is updated using these principles, just at a much larger scale.\n",
    "- Understanding this process is key to debugging and designing new architectures.\n",
    "\n",
    "In the next notebook, you'll use these gradients to train a network on a real task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
