{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 15 - Encoder-Decoder Architecture (No Attention)\n",
       "\n",
       "Encoder-decoder models are the foundation of many sequence-to-sequence tasks, such as translation and summarization. Before attention, these models used a fixed-size context vector to transfer information from the encoder to the decoder.\n",
       "\n",
       "In this notebook, you'll scaffold the components of a basic encoder-decoder model, and see how this structure leads to the transformer architecture in LLMs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ Encoder: Sequence to Context Vector\n",
       "\n",
       "The encoder processes the input sequence and produces a fixed-size context vector (usually the final hidden state).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- The encoder is analogous to the encoder stack in transformer-based models like BERT and T5.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run an RNN/LSTM/GRU encoder over an input sequence and return the final hidden state (context vector).\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def encoder_forward(X_seq, h0, encoder_params, cell_type='rnn'):\n",
       "    \"\"\"\n",
       "    Run the encoder over an input sequence to produce a context vector.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        encoder_params (dict): Encoder parameters (weights, biases, etc.)\n",
       "        cell_type (str): 'rnn', 'lstm', or 'gru'\n",
       "    Returns:\n",
       "        np.ndarray: Context vector (final hidden state)\n",
       "    \"\"\"\n",
       "    # TODO: Implement encoder forward pass for the chosen cell type\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Decoder: Context Vector to Output Sequence\n",
       "\n",
       "The decoder generates the output sequence, using the context vector as its initial hidden state.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- The decoder is analogous to the decoder stack in transformer-based models like GPT and T5.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run an RNN/LSTM/GRU decoder, generating an output sequence from the context vector.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def decoder_forward(context_vector, Y_seq, decoder_params, cell_type='rnn'):\n",
       "    \"\"\"\n",
       "    Run the decoder to generate an output sequence from the context vector.\n",
       "    Args:\n",
       "        context_vector (np.ndarray): Initial hidden state for the decoder (hidden_dim,)\n",
       "        Y_seq (np.ndarray): Output sequence input (seq_len x input_dim)\n",
       "        decoder_params (dict): Decoder parameters (weights, biases, etc.)\n",
       "        cell_type (str): 'rnn', 'lstm', or 'gru'\n",
       "    Returns:\n",
       "        np.ndarray: Output sequence logits (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Implement decoder forward pass for the chosen cell type\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Sequence-to-Sequence Training (No Attention)\n",
       "\n",
       "Train the encoder-decoder model by minimizing the loss between the predicted and true output sequences.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the basis for training translation, summarization, and other seq2seq models before attention and transformers.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute the total loss over the output sequence.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def seq2seq_loss(logits_seq, target_seq, loss_fn):\n",
       "    \"\"\"\n",
       "    Compute the total loss for the output sequence.\n",
       "    Args:\n",
       "        logits_seq (np.ndarray): Output logits (seq_len x output_dim)\n",
       "        target_seq (np.ndarray): True output indices (seq_len,)\n",
       "        loss_fn (callable): Loss function (e.g., cross-entropy)\n",
       "    Returns:\n",
       "        float: Total loss over the sequence\n",
       "    \"\"\"\n",
       "    # TODO: Compute total sequence loss\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: Encoder-Decoder and LLMs\n",
       "\n",
       "- Encoder-decoder models are the foundation of many sequence-to-sequence tasks.\n",
       "- Before attention, these models relied on a fixed-size context vector, which limited their ability to handle long sequences.\n",
       "- Transformers solve this with self-attention, allowing the model to access all input positions at every decoding step.\n",
       "\n",
       "In the next notebook, you'll add attention to the encoder-decoder architecture, paving the way to transformers!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   