{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 18 - Teacher Forcing in Sequence Models\n",
       "\n",
       "Teacher forcing is a training technique for sequence-to-sequence models where the true previous output is fed as input to the decoder, rather than the model's own prediction. This helps models learn faster and more stably, especially in early training.\n",
       "\n",
       "In this notebook, you'll scaffold the logic for teacher forcing and see how it is used in LLMs and transformers."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ What is Teacher Forcing?\n",
       "\n",
       "During training, the decoder receives the ground-truth previous token as input at each step, rather than its own previous prediction.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Teacher forcing is used in training encoder-decoder models (including transformers) for tasks like translation and summarization.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to perform a decoder forward pass with teacher forcing.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def decoder_forward_teacher_forcing(context_vector, target_seq, decoder_params, cell_type='rnn'):\n",
       "    \"\"\"\n",
       "    Run the decoder with teacher forcing: at each step, feed the true previous token as input.\n",
       "    Args:\n",
       "        context_vector (np.ndarray): Initial hidden state for the decoder (hidden_dim,)\n",
       "        target_seq (np.ndarray): Ground-truth output sequence (seq_len x input_dim)\n",
       "        decoder_params (dict): Decoder parameters (weights, biases, etc.)\n",
       "        cell_type (str): 'rnn', 'lstm', or 'gru'\n",
       "    Returns:\n",
       "        np.ndarray: Output logits for each step (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Implement decoder forward pass with teacher forcing\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Scheduled Sampling (Optional Extension)\n",
       "\n",
       "Scheduled sampling is a technique where, during training, the model sometimes uses its own prediction as the next input instead of the ground-truth token. This helps bridge the gap between training and inference.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Scheduled sampling can help models become more robust to their own mistakes during generation.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for scheduled sampling, where you probabilistically choose between the true token and the model's prediction at each step.\n",
       "- Add a docstring explaining its use."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def decoder_forward_scheduled_sampling(context_vector, target_seq, decoder_params, sampling_prob, cell_type='rnn'):\n",
       "    \"\"\"\n",
       "    Run the decoder with scheduled sampling: probabilistically use ground-truth or model prediction as next input.\n",
       "    Args:\n",
       "        context_vector (np.ndarray): Initial hidden state for the decoder (hidden_dim,)\n",
       "        target_seq (np.ndarray): Ground-truth output sequence (seq_len x input_dim)\n",
       "        decoder_params (dict): Decoder parameters (weights, biases, etc.)\n",
       "        sampling_prob (float): Probability of using model prediction as next input.\n",
       "        cell_type (str): 'rnn', 'lstm', or 'gru'\n",
       "    Returns:\n",
       "        np.ndarray: Output logits for each step (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Implement decoder forward pass with scheduled sampling\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: Teacher Forcing in LLMs\n",
       "\n",
       "- Teacher forcing accelerates training and stabilizes sequence models by providing the correct context at each step.\n",
       "- Scheduled sampling helps bridge the gap between training (teacher forcing) and inference (autoregressive generation).\n",
       "- These techniques are widely used in training encoder-decoder LLMs and transformers for sequence generation tasks.\n",
       "\n",
       "In the next notebook, you'll explore self-attention and the transformer architecture!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   