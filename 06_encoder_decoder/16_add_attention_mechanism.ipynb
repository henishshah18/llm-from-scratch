{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 16 - Adding Attention Mechanism to Encoder-Decoder\n",
       "\n",
       "Attention mechanisms allow models to focus on different parts of the input sequence when generating each output. This solves the bottleneck of fixed-size context vectors and is the key innovation that led to transformers and modern LLMs.\n",
       "\n",
       "In this notebook, you'll scaffold the steps to add attention to an encoder-decoder model, building up to the core of the transformer architecture."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîé What is Attention?\n",
       "\n",
       "Attention computes a weighted sum of encoder hidden states, where the weights are determined by the similarity between the current decoder state and each encoder state.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Attention allows the model to access all input positions at every decoding step, enabling long-range dependencies and flexible context.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute attention weights given a decoder hidden state and all encoder hidden states.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def compute_attention_weights(decoder_hidden, encoder_hiddens):\n",
       "    \"\"\"\n",
       "    Compute attention weights for the decoder hidden state over all encoder hidden states.\n",
       "    Args:\n",
       "        decoder_hidden (np.ndarray): Current decoder hidden state (hidden_dim,)\n",
       "        encoder_hiddens (np.ndarray): All encoder hidden states (seq_len x hidden_dim)\n",
       "    Returns:\n",
       "        np.ndarray: Attention weights (seq_len,)\n",
       "    \"\"\"\n",
       "    # TODO: Compute similarity scores and normalize (e.g., softmax)\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîó Context Vector with Attention\n",
       "\n",
       "The context vector for the decoder at each step is a weighted sum of encoder hidden states, using the attention weights.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the core of the attention mechanism in transformers, allowing dynamic context for each output position.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute the context vector using attention weights and encoder hidden states.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def compute_context_vector(attn_weights, encoder_hiddens):\n",
       "    \"\"\"\n",
       "    Compute the context vector as a weighted sum of encoder hidden states.\n",
       "    Args:\n",
       "        attn_weights (np.ndarray): Attention weights (seq_len,)\n",
       "        encoder_hiddens (np.ndarray): Encoder hidden states (seq_len x hidden_dim)\n",
       "    Returns:\n",
       "        np.ndarray: Context vector (hidden_dim,)\n",
       "    \"\"\"\n",
       "    # TODO: Compute weighted sum\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Decoder with Attention\n",
       "\n",
       "At each decoding step, the decoder uses both its previous hidden state and the context vector from attention to generate the next output.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the precursor to the multi-head self-attention in transformers.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for a decoder step that incorporates the context vector from attention.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def decoder_step_with_attention(y_prev, h_prev, context_vector, decoder_params):\n",
       "    \"\"\"\n",
       "    Perform one decoder step using the previous output, previous hidden state, and context vector.\n",
       "    Args:\n",
       "        y_prev (np.ndarray): Previous output/input to decoder (input_dim,)\n",
       "        h_prev (np.ndarray): Previous decoder hidden state (hidden_dim,)\n",
       "        context_vector (np.ndarray): Context vector from attention (hidden_dim,)\n",
       "        decoder_params (dict): Decoder parameters (weights, biases, etc.)\n",
       "    Returns:\n",
       "        tuple: (new_hidden, output_logits)\n",
       "    \"\"\"\n",
       "    # TODO: Implement decoder step with context vector\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ Full Encoder-Decoder with Attention\n",
       "\n",
       "Combine the encoder, attention mechanism, and decoder to process an input sequence and generate an output sequence.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This structure is the direct precursor to the transformer encoder-decoder architecture used in LLMs like T5 and BART.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run the full encoder-decoder with attention for a sequence-to-sequence task.\n",
       "- Add a docstring explaining the workflow."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def encoder_decoder_with_attention(X_seq, Y_seq, encoder_params, decoder_params):\n",
       "    \"\"\"\n",
       "    Run the full encoder-decoder model with attention for a sequence-to-sequence task.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        Y_seq (np.ndarray): Output sequence input (seq_len x input_dim)\n",
       "        encoder_params (dict): Encoder parameters\n",
       "        decoder_params (dict): Decoder parameters\n",
       "    Returns:\n",
       "        np.ndarray: Output logits for each output position (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the full encoder-decoder with attention\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: Attention and LLMs\n",
       "\n",
       "- Attention mechanisms allow models to dynamically focus on relevant parts of the input, solving the bottleneck of fixed-size context vectors.\n",
       "- This innovation led directly to the transformer architecture, which is the foundation of all modern LLMs.\n",
       "- Mastering attention is key to understanding how LLMs process and generate language.\n",
       "\n",
       "In the next notebook, you'll explore positional encoding, which allows transformers to model order in sequences!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   