{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 17 - Positional Encoding in Transformers\n",
       "\n",
       "Transformers do not have any built-in notion of sequence order. Positional encoding injects information about the position of each token, enabling the model to capture order and structure in sequences.\n",
       "\n",
       "In this notebook, you'll scaffold the core logic of positional encoding, and see how it is used in LLMs and transformers."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ Why Positional Encoding?\n",
       "\n",
       "Self-attention treats all tokens as a set, so we must add position information to the input embeddings.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Every transformer-based LLM (e.g., GPT, BERT) uses positional encoding to model word order.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to add positional encodings to input embeddings.\n",
       "- Add a docstring explaining why this is necessary."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def add_positional_encoding(embeddings, pos_encodings):\n",
       "    \"\"\"\n",
       "    Add positional encodings to input embeddings.\n",
       "    Args:\n",
       "        embeddings (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        pos_encodings (np.ndarray): Positional encodings (seq_len x d_model)\n",
       "    Returns:\n",
       "        np.ndarray: Position-aware embeddings (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Add positional encodings to embeddings\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Sinusoidal Positional Encoding (Vaswani et al.)\n",
       "\n",
       "The original transformer uses fixed sinusoidal functions to encode positions, allowing the model to extrapolate to longer sequences.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Sinusoidal encodings are used in many transformer models and are a standard baseline.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to generate sinusoidal positional encodings for a given sequence length and embedding dimension.\n",
       "- Add a docstring explaining the formula and its properties."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def sinusoidal_positional_encoding(seq_len, d_model):\n",
       "    \"\"\"\n",
       "    Generate sinusoidal positional encodings (as in the original transformer).\n",
       "    Args:\n",
       "        seq_len (int): Length of the sequence.\n",
       "        d_model (int): Embedding dimension.\n",
       "    Returns:\n",
       "        np.ndarray: Positional encodings (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement sinusoidal positional encoding\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Learnable Positional Embeddings\n",
       "\n",
       "Many modern LLMs use learnable positional embeddings instead of fixed encodings, allowing the model to adapt position representations during training.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Learnable positional embeddings are used in GPT, BERT, and most large-scale LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to initialize learnable positional embeddings.\n",
       "- Add a docstring explaining their use."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def init_learnable_positional_embeddings(seq_len, d_model):\n",
       "    \"\"\"\n",
       "    Initialize learnable positional embeddings.\n",
       "    Args:\n",
       "        seq_len (int): Length of the sequence.\n",
       "        d_model (int): Embedding dimension.\n",
       "    Returns:\n",
       "        np.ndarray: Learnable positional embeddings (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Initialize learnable positional embeddings (e.g., random or zeros)\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: Positional Encoding in LLMs\n",
       "\n",
       "- Positional encoding is essential for transformers to model sequence order.\n",
       "- Both fixed (sinusoidal) and learnable encodings are used in LLMs.\n",
       "- Understanding positional encoding is key to building and interpreting transformer models.\n",
       "\n",
       "In the next notebook, you'll see how positional encoding is integrated into the full transformer block!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   