{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 21 - Positional Encoding in the Transformer\n",
       "\n",
       "Transformers require positional encoding to model the order of tokens, since self-attention alone is permutation-invariant. In this notebook, you'll scaffold the integration of positional encoding into the transformer block, as used in LLMs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ Adding Positional Encoding to Input Embeddings\n",
       "\n",
       "Before any transformer block, positional encodings are added to the input token embeddings.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This step is essential for all transformer-based LLMs (e.g., GPT, BERT, T5).\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to add positional encodings to input embeddings.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def add_positional_encoding_to_embeddings(embeddings, pos_encodings):\n",
       "    \"\"\"\n",
       "    Add positional encodings to input embeddings before the transformer block.\n",
       "    Args:\n",
       "        embeddings (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        pos_encodings (np.ndarray): Positional encodings (seq_len x d_model)\n",
       "    Returns:\n",
       "        np.ndarray: Position-aware embeddings (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Add positional encodings to embeddings\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Sinusoidal vs. Learnable Positional Encodings\n",
       "\n",
       "Transformers can use either fixed (sinusoidal) or learnable positional encodings. Each has trade-offs in generalization and flexibility.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- GPT and BERT use learnable encodings; the original transformer used sinusoidal.\n",
       "\n",
       "### Task:\n",
       "- Scaffold code to select and initialize either sinusoidal or learnable positional encodings for a given sequence length and embedding dimension.\n",
       "- Add comments explaining the choice."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# TODO: Choose and initialize positional encodings (sinusoidal or learnable)\n",
       "pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Integrating Positional Encoding in the Transformer Block\n",
       "\n",
       "After adding positional encodings, the embeddings are passed through the transformer block (multi-head attention, feedforward, etc.).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This integration is the first step in every transformer layer in LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to perform the full input step: embedding lookup, add positional encoding, and pass to the transformer block.\n",
       "- Add a docstring explaining the workflow."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def transformer_input_step(token_indices, embedding_matrix, pos_encodings, transformer_block_fn):\n",
       "    \"\"\"\n",
       "    Full input step for a transformer: embedding lookup, add positional encoding, pass to transformer block.\n",
       "    Args:\n",
       "        token_indices (np.ndarray): Sequence of token indices (seq_len,)\n",
       "        embedding_matrix (np.ndarray): Token embedding matrix (vocab_size x d_model)\n",
       "        pos_encodings (np.ndarray): Positional encodings (seq_len x d_model)\n",
       "        transformer_block_fn (callable): Function for the transformer block\n",
       "    Returns:\n",
       "        np.ndarray: Output of the transformer block (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the full input step for the transformer\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: Positional Encoding in LLMs\n",
       "\n",
       "- Adding positional encoding to input embeddings is essential for transformers to model sequence order.\n",
       "- Both fixed and learnable encodings are used in LLMs, with different trade-offs.\n",
       "- Mastering this integration is key to building and understanding transformer-based LLMs.\n",
       "\n",
       "In the next notebook, you'll build the full transformer block from scratch!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   