{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 19 - Self-Attention from Scratch\n",
       "\n",
       "Self-attention is the core mechanism behind transformers and LLMs. It allows each token to attend to every other token in the sequence, enabling the model to capture long-range dependencies and context.\n",
       "\n",
       "In this notebook, you'll scaffold the steps to implement self-attention from scratch, building up to the transformer block."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ What is Self-Attention?\n",
       "\n",
       "Self-attention computes a weighted sum of all token representations in a sequence, where the weights are determined by the similarity between tokens.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Self-attention enables transformers to model relationships between all tokens, regardless of their distance in the sequence.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute self-attention scores (dot-product attention) for a batch of token embeddings.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def compute_self_attention_scores(Q, K):\n",
       "    \"\"\"\n",
       "    Compute self-attention scores (dot products) between all query and key vectors.\n",
       "    Args:\n",
       "        Q (np.ndarray): Query matrix (seq_len x d_k)\n",
       "        K (np.ndarray): Key matrix (seq_len x d_k)\n",
       "    Returns:\n",
       "        np.ndarray: Attention scores (seq_len x seq_len)\n",
       "    \"\"\"\n",
       "    # TODO: Compute dot-product attention scores\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Softmax Normalization\n",
       "\n",
       "The raw attention scores are normalized with softmax to produce attention weights (probabilities).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Softmax ensures the attention weights sum to 1 for each token, allowing the model to focus on the most relevant tokens.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to apply softmax to the attention scores along the correct axis.\n",
       "- Add a docstring explaining its use."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def softmax_attention_weights(attn_scores):\n",
       "    \"\"\"\n",
       "    Apply softmax to attention scores to get attention weights.\n",
       "    Args:\n",
       "        attn_scores (np.ndarray): Raw attention scores (seq_len x seq_len)\n",
       "    Returns:\n",
       "        np.ndarray: Attention weights (seq_len x seq_len)\n",
       "    \"\"\"\n",
       "    # TODO: Apply softmax along the correct axis\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Weighted Sum: Computing the Output\n",
       "\n",
       "The output of self-attention is a weighted sum of the value vectors, using the attention weights.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This step produces the new representation for each token, incorporating information from the entire sequence.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute the weighted sum of value vectors using the attention weights.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def compute_attention_output(attn_weights, V):\n",
       "    \"\"\"\n",
       "    Compute the output of self-attention as a weighted sum of value vectors.\n",
       "    Args:\n",
       "        attn_weights (np.ndarray): Attention weights (seq_len x seq_len)\n",
       "        V (np.ndarray): Value matrix (seq_len x d_v)\n",
       "    Returns:\n",
       "        np.ndarray: Self-attention output (seq_len x d_v)\n",
       "    \"\"\"\n",
       "    # TODO: Compute weighted sum of value vectors\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Full Self-Attention Layer\n",
       "\n",
       "Combine the steps: project input embeddings to Q, K, V, compute attention scores, normalize, and compute the output.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the core computation in every transformer block in LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the full self-attention layer (single head).\n",
       "- Add a docstring explaining the workflow."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def self_attention_layer(X, W_q, W_k, W_v):\n",
       "    \"\"\"\n",
       "    Full self-attention layer (single head): project to Q, K, V, compute attention, and output.\n",
       "    Args:\n",
       "        X (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        W_q, W_k, W_v (np.ndarray): Projection matrices (d_model x d_k)\n",
       "    Returns:\n",
       "        np.ndarray: Self-attention output (seq_len x d_k)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the full self-attention layer\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: Self-Attention and LLMs\n",
       "\n",
       "- Self-attention is the key innovation that enables transformers and LLMs to model complex, long-range dependencies in language.\n",
       "- Every token can attend to every other token, allowing for rich contextual representations.\n",
       "- Mastering self-attention is essential for understanding and building transformer-based LLMs.\n",
       "\n",
       "In the next notebook, you'll extend this to multi-head attention, a core feature of modern transformers!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   