{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 22 - Transformer Block from Scratch\n",
       "\n",
       "The transformer block is the fundamental building unit of all modern LLMs. It combines multi-head self-attention, residual connections, layer normalization, and a feedforward network.\n",
       "\n",
       "In this notebook, you'll scaffold the steps to build a transformer block, as used in GPT, BERT, and other LLMs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ Multi-Head Self-Attention Layer\n",
       "\n",
       "The first step in a transformer block is multi-head self-attention, which allows each token to attend to all others in the sequence.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the core mechanism for capturing relationships between tokens in LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute multi-head self-attention for input embeddings.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def multihead_self_attention(X, params):\n",
       "    \"\"\"\n",
       "    Compute multi-head self-attention for input embeddings.\n",
       "    Args:\n",
       "        X (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        params (dict): All attention parameters (projection matrices, etc.)\n",
       "    Returns:\n",
       "        np.ndarray: Output of multi-head attention (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement multi-head self-attention\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Residual Connection and Layer Normalization (Post-Attention)\n",
       "\n",
       "After self-attention, a residual connection and layer normalization are applied.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Residuals and normalization stabilize training and enable deep stacking of transformer blocks.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to add a residual connection and apply layer normalization after attention.\n",
       "- Add a docstring explaining its importance."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def add_residual_and_layernorm(X, sublayer_output, layernorm_params):\n",
       "    \"\"\"\n",
       "    Add a residual connection and apply layer normalization.\n",
       "    Args:\n",
       "        X (np.ndarray): Input to the sublayer (seq_len x d_model)\n",
       "        sublayer_output (np.ndarray): Output from the sublayer (seq_len x d_model)\n",
       "        layernorm_params (dict): Parameters for layer normalization (gamma, beta)\n",
       "    Returns:\n",
       "        np.ndarray: Output after residual and layer normalization (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Add residual and apply layer normalization\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Feedforward Network (FFN)\n",
       "\n",
       "The feedforward network consists of two linear layers with a non-linearity (usually GELU or ReLU) in between.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- The FFN increases the model's capacity and is applied independently to each token position.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the position-wise feedforward network.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def transformer_feedforward(X, params):\n",
       "    \"\"\"\n",
       "    Position-wise feedforward network in the transformer block.\n",
       "    Args:\n",
       "        X (np.ndarray): Input (seq_len x d_model)\n",
       "        params (dict): FFN parameters (weights, biases)\n",
       "    Returns:\n",
       "        np.ndarray: Output of the feedforward network (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the feedforward network\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Residual Connection and Layer Normalization (Post-FFN)\n",
       "\n",
       "A second residual connection and layer normalization are applied after the feedforward network.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This further stabilizes training and enables deep stacking.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to add a residual connection and apply layer normalization after the FFN.\n",
       "- Add a docstring explaining its importance."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def add_residual_and_layernorm_ffn(X, ffn_output, layernorm_params):\n",
       "    \"\"\"\n",
       "    Add a residual connection and apply layer normalization after the FFN.\n",
       "    Args:\n",
       "        X (np.ndarray): Input to the FFN (seq_len x d_model)\n",
       "        ffn_output (np.ndarray): Output from the FFN (seq_len x d_model)\n",
       "        layernorm_params (dict): Parameters for layer normalization (gamma, beta)\n",
       "    Returns:\n",
       "        np.ndarray: Output after residual and layer normalization (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Add residual and apply layer normalization\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§± Full Transformer Block\n",
       "\n",
       "Combine all steps: multi-head attention, residual + layer norm, feedforward, residual + layer norm.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the core building block of all modern LLMs and is stacked many times in deep models.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the full transformer block.\n",
       "- Add a docstring explaining the workflow."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def transformer_block(X, attn_params, ffn_params, ln1_params, ln2_params):\n",
       "    \"\"\"\n",
       "    Full transformer block: multi-head attention, residual + layer norm, feedforward, residual + layer norm.\n",
       "    Args:\n",
       "        X (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        attn_params (dict): Attention parameters\n",
       "        ffn_params (dict): Feedforward network parameters\n",
       "        ln1_params (dict): Layer norm parameters after attention\n",
       "        ln2_params (dict): Layer norm parameters after FFN\n",
       "    Returns:\n",
       "        np.ndarray: Output of the transformer block (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the full transformer block\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: The Transformer Block in LLMs\n",
       "\n",
       "- The transformer block is the fundamental unit of all modern LLMs, combining attention, feedforward, normalization, and residuals.\n",
       "- Stacking these blocks enables models to learn deep, contextual representations of language.\n",
       "- Mastering the transformer block is essential for building and understanding LLMs.\n",
       "\n",
       "In the next notebook, you'll use this block to build a mini transformer language model!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   