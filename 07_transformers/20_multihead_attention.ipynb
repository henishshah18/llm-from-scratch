{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 20 - Multi-Head Attention from Scratch\n",
       "\n",
       "Multi-head attention is a core component of transformers and LLMs. It allows the model to attend to information from different representation subspaces at different positions, greatly increasing the model's expressivity and power.\n",
       "\n",
       "In this notebook, you'll scaffold the steps to implement multi-head self-attention, building up to the full transformer block."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ What is Multi-Head Attention?\n",
       "\n",
       "Multi-head attention runs several self-attention operations (\"heads\") in parallel, each with its own set of learned projections. The outputs are concatenated and projected to the final output space.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Multi-head attention enables transformers to capture diverse relationships and patterns in the input sequence, which is critical for LLM performance.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to split input embeddings into multiple heads and project to Q, K, V for each head.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def split_heads(X, num_heads):\n",
       "    \"\"\"\n",
       "    Split input embeddings into multiple heads for multi-head attention.\n",
       "    Args:\n",
       "        X (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        num_heads (int): Number of attention heads\n",
       "    Returns:\n",
       "        np.ndarray: Split embeddings (num_heads x seq_len x d_head)\n",
       "    \"\"\"\n",
       "    # TODO: Split X into num_heads along the last dimension\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Per-Head Self-Attention\n",
       "\n",
       "Each head performs self-attention independently on its projected Q, K, V.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Each head can focus on different types of relationships in the sequence.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute self-attention for each head.\n",
       "- Add a docstring explaining the per-head computation."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def multihead_attention_per_head(Q_heads, K_heads, V_heads):\n",
       "    \"\"\"\n",
       "    Compute self-attention for each head independently.\n",
       "    Args:\n",
       "        Q_heads, K_heads, V_heads (np.ndarray): (num_heads x seq_len x d_head)\n",
       "    Returns:\n",
       "        np.ndarray: Attention outputs for each head (num_heads x seq_len x d_head)\n",
       "    \"\"\"\n",
       "    # TODO: Compute self-attention for each head\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Concatenation and Output Projection\n",
       "\n",
       "The outputs of all heads are concatenated and projected to the final output dimension.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This step combines information from all heads, allowing the model to integrate multiple perspectives.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to concatenate head outputs and apply the final linear projection.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def combine_heads_and_project(head_outputs, W_o):\n",
       "    \"\"\"\n",
       "    Concatenate outputs from all heads and project to the output dimension.\n",
       "    Args:\n",
       "        head_outputs (np.ndarray): (num_heads x seq_len x d_head)\n",
       "        W_o (np.ndarray): Output projection matrix (d_model x d_model)\n",
       "    Returns:\n",
       "        np.ndarray: Final multi-head attention output (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Concatenate and project head outputs\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Full Multi-Head Attention Layer\n",
       "\n",
       "Combine all steps: split heads, project Q/K/V, compute per-head attention, concatenate, and project.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the core of every transformer block in LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the full multi-head attention layer.\n",
       "- Add a docstring explaining the workflow."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def multihead_attention_layer(X, W_q, W_k, W_v, W_o, num_heads):\n",
       "    \"\"\"\n",
       "    Full multi-head attention layer: split heads, project Q/K/V, compute attention, combine, and project.\n",
       "    Args:\n",
       "        X (np.ndarray): Input embeddings (seq_len x d_model)\n",
       "        W_q, W_k, W_v (np.ndarray): Projection matrices (d_model x d_model)\n",
       "        W_o (np.ndarray): Output projection matrix (d_model x d_model)\n",
       "        num_heads (int): Number of attention heads\n",
       "    Returns:\n",
       "        np.ndarray: Multi-head attention output (seq_len x d_model)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the full multi-head attention layer\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: Multi-Head Attention in LLMs\n",
       "\n",
       "- Multi-head attention is the key to the power and flexibility of transformers and LLMs.\n",
       "- It allows the model to capture diverse relationships and integrate information from multiple perspectives.\n",
       "- Mastering multi-head attention is essential for understanding and building transformer-based LLMs.\n",
       "\n",
       "In the next notebook, you'll see how positional encoding and multi-head attention are combined in the full transformer block!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   