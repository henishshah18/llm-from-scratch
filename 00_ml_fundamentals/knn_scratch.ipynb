{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) from Scratch\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, non-parametric algorithm for classification and regression. It predicts the label of a sample based on the majority label (or average value) of its k nearest neighbors in the feature space.\n",
    "\n",
    "In this notebook, you'll scaffold the steps to implement KNN from scratch, including distance metrics, prediction, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Distance Metrics\n",
    "\n",
    "KNN relies on a distance metric to find the closest neighbors. Common choices are Euclidean and Manhattan distance.\n",
    "\n",
    "### Task:\n",
    "- Scaffold functions to compute Euclidean and Manhattan distances between two points.\n",
    "- Add docstrings explaining their use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two points.\n",
    "    Args:\n",
    "        x1, x2 (np.ndarray): Input vectors.\n",
    "    Returns:\n",
    "        float: Euclidean distance.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Euclidean distance\n",
    "    pass\n",
    "\n",
    "def manhattan_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Compute the Manhattan distance between two points.\n",
    "    Args:\n",
    "        x1, x2 (np.ndarray): Input vectors.\n",
    "    Returns:\n",
    "        float: Manhattan distance.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Manhattan distance\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Finding the K Nearest Neighbors\n",
    "\n",
    "For a given sample, find the k closest points in the training set using the chosen distance metric.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to find the indices of the k nearest neighbors.\n",
    "- Add a docstring explaining its use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_k_nearest_neighbors(X_train, x_query, k, distance_fn):\n",
    "    \"\"\"\n",
    "    Find the indices of the k nearest neighbors in X_train to x_query.\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data (n_samples x n_features).\n",
    "        x_query (np.ndarray): Query point (n_features,).\n",
    "        k (int): Number of neighbors.\n",
    "        distance_fn (callable): Distance function.\n",
    "    Returns:\n",
    "        list: Indices of the k nearest neighbors.\n",
    "    \"\"\"\n",
    "    # TODO: Find k nearest neighbors\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑ KNN Classification\n",
    "\n",
    "Predict the class label for a query point by majority vote among its k nearest neighbors.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to predict the class label for a query point.\n",
    "- Add a docstring explaining its use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def knn_predict_class(X_train, y_train, x_query, k, distance_fn):\n",
    "    \"\"\"\n",
    "    Predict the class label for x_query using KNN.\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        x_query (np.ndarray): Query point.\n",
    "        k (int): Number of neighbors.\n",
    "        distance_fn (callable): Distance function.\n",
    "    Returns:\n",
    "        int or str: Predicted class label.\n",
    "    \"\"\"\n",
    "    # TODO: Predict class label using majority vote\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ KNN Regression (Optional)\n",
    "\n",
    "For regression, predict the value as the average of the k nearest neighbors' values.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to predict a regression value for a query point.\n",
    "- Add a docstring explaining its use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def knn_predict_regression(X_train, y_train, x_query, k, distance_fn):\n",
    "    \"\"\"\n",
    "    Predict the regression value for x_query using KNN.\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data.\n",
    "        y_train (np.ndarray): Training values.\n",
    "        x_query (np.ndarray): Query point.\n",
    "        k (int): Number of neighbors.\n",
    "        distance_fn (callable): Distance function.\n",
    "    Returns:\n",
    "        float: Predicted value.\n",
    "    \"\"\"\n",
    "    # TODO: Predict regression value using average of neighbors\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Feature Scaling / Normalization\n",
    "\n",
    "Feature scaling is important for KNN, as distance metrics are sensitive to the scale of features.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to normalize features (e.g., min-max or z-score normalization).\n",
    "- Add a docstring explaining its use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def normalize_features(X):\n",
    "    \"\"\"\n",
    "    Normalize features (e.g., min-max or z-score normalization).\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "    Returns:\n",
    "        np.ndarray: Normalized features.\n",
    "    \"\"\"\n",
    "    # TODO: Normalize features\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèã Training and Evaluation Loop\n",
    "\n",
    "KNN is a lazy learner: no explicit training, but you can evaluate its performance on a test set.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compute accuracy for classification or MSE for regression.\n",
    "- Add docstrings explaining their use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_accuracy_knn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy for KNN classification.\n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels.\n",
    "        y_pred (np.ndarray): Predicted labels.\n",
    "    Returns:\n",
    "        float: Accuracy (0 to 1).\n",
    "    \"\"\"\n",
    "    # TODO: Compute accuracy\n",
    "    pass\n",
    "\n",
    "def compute_mse_knn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute mean squared error for KNN regression.\n",
    "    Args:\n",
    "        y_true (np.ndarray): True values.\n",
    "        y_pred (np.ndarray): Predicted values.\n",
    "    Returns:\n",
    "        float: Mean squared error.\n",
    "    \"\"\"\n",
    "    # TODO: Compute MSE\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Final Summary: KNN in ML\n",
    "\n",
    "- KNN is a simple, interpretable, and effective algorithm for both classification and regression.\n",
    "- Understanding distance metrics, feature scaling, and the curse of dimensionality is key for using KNN effectively.\n",
    "- KNN is a useful baseline and a great way to build intuition for more complex ML models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
