{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 11 - RNN Character-Level Language Model\n",
       "\n",
       "A character-level language model predicts the next character in a sequence, one character at a time. This is a foundational idea for LLMs, which predict the next token (word or subword) in a sequence.\n",
       "\n",
       "In this notebook, you'll scaffold the components needed to build, train, and sample from a simple RNN-based character-level language model."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üìö Preparing the Dataset\n",
       "\n",
       "To train a character-level model, you need to encode text as sequences of integer indices (one per character).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- LLMs use tokenization to convert text into sequences of integers. Here, we use characters as tokens.\n",
       "\n",
       "### Task:\n",
       "- Scaffold code to build a character vocabulary and encode text as integer sequences.\n",
       "- Add comments explaining each step."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# TODO: Build char-to-index and index-to-char vocabularies\n",
       "# TODO: Encode a sample text as a sequence of integer indices\n",
       "pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üî¢ Input/Target Sequence Preparation\n",
       "\n",
       "For language modeling, the input is a sequence of characters, and the target is the same sequence shifted by one character (the next character at each position).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the same as next-token prediction in LLMs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold code to create input and target sequences for training."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# TODO: Given an encoded text, create input and target sequences for training\n",
       "pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ RNN Language Model Architecture\n",
       "\n",
       "The model consists of an embedding layer, an RNN, and an output layer mapping to vocabulary logits.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- LLMs use token embeddings, deep transformer blocks, and output logits for next-token prediction. Here, we use a single RNN layer for simplicity.\n",
       "\n",
       "### Task:\n",
       "- Scaffold functions to initialize model parameters: embedding matrix, RNN weights, and output weights."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def init_char_rnn_model(vocab_size, embed_dim, hidden_dim):\n",
       "    \"\"\"\n",
       "    Initialize parameters for a character-level RNN language model.\n",
       "    Args:\n",
       "        vocab_size (int): Number of unique characters.\n",
       "        embed_dim (int): Embedding dimension.\n",
       "        hidden_dim (int): RNN hidden state dimension.\n",
       "    Returns:\n",
       "        dict: Model parameters (embedding, W_xh, W_hh, b_h, W_hy, b_y)\n",
       "    \"\"\"\n",
       "    # TODO: Initialize all model parameters\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîó Forward Pass: Embedding, RNN, Output\n",
       "\n",
       "The forward pass consists of embedding lookup, RNN unrolling, and output logits computation for each time step.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This mirrors the forward pass in LLMs, just at the character level and with a single RNN layer.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the forward pass through the model, returning logits for each time step."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def char_rnn_forward(input_seq, h0, params):\n",
       "    \"\"\"\n",
       "    Forward pass for a character-level RNN language model.\n",
       "    Args:\n",
       "        input_seq (np.ndarray): Sequence of input character indices (seq_len,)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        params (dict): Model parameters\n",
       "    Returns:\n",
       "        logits_seq (np.ndarray): Logits for each time step (seq_len x vocab_size)\n",
       "        h_states (list): Hidden states for each time step\n",
       "    \"\"\"\n",
       "    # TODO: Implement the forward pass (embedding -> RNN -> output)\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Loss Computation: Cross-Entropy\n",
       "\n",
       "Compute the cross-entropy loss between the predicted logits and the true next character at each time step.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the same loss used in LLMs for next-token prediction.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute the average cross-entropy loss over a sequence."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def sequence_cross_entropy_loss(logits_seq, target_seq):\n",
       "    \"\"\"\n",
       "    Compute average cross-entropy loss over a sequence.\n",
       "    Args:\n",
       "        logits_seq (np.ndarray): Logits for each time step (seq_len x vocab_size)\n",
       "        target_seq (np.ndarray): True next character indices (seq_len,)\n",
       "    Returns:\n",
       "        float: Average loss\n",
       "    \"\"\"\n",
       "    # TODO: Implement sequence cross-entropy loss\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ Sampling: Generating Text\n",
       "\n",
       "After training, you can sample new text by feeding the model's output back as input.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is how LLMs generate text, one token at a time.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to sample a sequence of characters from the trained model, given a start character."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def sample_char_rnn(start_char_idx, params, char_to_idx, idx_to_char, sample_length):\n",
       "    \"\"\"\n",
       "    Sample a sequence of characters from the trained RNN model.\n",
       "    Args:\n",
       "        start_char_idx (int): Index of the start character.\n",
       "        params (dict): Trained model parameters.\n",
       "        char_to_idx (dict): Character to index mapping.\n",
       "        idx_to_char (dict): Index to character mapping.\n",
       "        sample_length (int): Number of characters to sample.\n",
       "    Returns:\n",
       "        str: Generated text sequence.\n",
       "    \"\"\"\n",
       "    # TODO: Implement text sampling from the RNN\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: Character-Level Language Modeling and LLMs\n",
       "\n",
       "- Character-level language models are a simple but powerful way to understand sequence modeling and next-token prediction.\n",
       "- LLMs use the same principles, but with subword/word tokens, deep transformer blocks, and much larger vocabularies.\n",
       "- Mastering this workflow gives you a strong foundation for building and understanding LLMs.\n",
       "\n",
       "In the next notebook, you'll explore more advanced recurrent architectures like LSTM and GRU!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   