{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 10 - RNN: Backpropagation Through Time (BPTT)\n",
       "\n",
       "Backpropagation Through Time (BPTT) is the algorithm used to train RNNs on sequences. It extends the chain rule to unroll the RNN over time, allowing gradients to flow through each time step.\n",
       "\n",
       "While transformers use attention instead of recurrence, understanding BPTT is key to grasping how sequence models learn dependencies across time steps."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ What is BPTT?\n",
       "\n",
       "BPTT unrolls the RNN across all time steps, computes the loss at each step, and then backpropagates gradients through the entire sequence.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- BPTT is the precursor to the parallel, attention-based backpropagation in transformers.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to unroll an RNN and store all intermediate activations needed for backpropagation.\n",
       "- Add a docstring explaining its role."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_forward_cache(X_seq, h0, W_xh, W_hh, b_h):\n",
       "    \"\"\"\n",
       "    Run an RNN over a sequence and cache all activations for BPTT.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        W_xh, W_hh, b_h: RNN parameters\n",
       "    Returns:\n",
       "        dict: Cached activations (inputs, hidden states, pre-activations)\n",
       "    \"\"\"\n",
       "    # TODO: Implement forward pass with caching for BPTT\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîó Backward Pass: Computing Gradients Through Time\n",
       "\n",
       "The backward pass computes gradients for all parameters by propagating errors backward through each time step.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is analogous to how gradients flow through the layers and positions in a transformer.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute gradients for all RNN parameters using cached activations and sequence loss gradients.\n",
       "- Add a docstring explaining the process."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_bptt(cache, dL_dh_last, W_xh, W_hh):\n",
       "    \"\"\"\n",
       "    Perform backpropagation through time (BPTT) for an RNN.\n",
       "    Args:\n",
       "        cache (dict): Cached activations from forward pass.\n",
       "        dL_dh_last (np.ndarray): Gradient of loss w.r.t. last hidden state.\n",
       "        W_xh, W_hh: RNN parameters (needed for gradients).\n",
       "    Returns:\n",
       "        dict: Gradients for W_xh, W_hh, b_h, and input sequence.\n",
       "    \"\"\"\n",
       "    # TODO: Implement BPTT to compute gradients for all parameters\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Gradient Clipping\n",
       "\n",
       "RNNs can suffer from exploding gradients during BPTT. Gradient clipping is used to prevent this by capping gradients at a maximum value.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- Gradient clipping is also used in transformer training to stabilize optimization.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to clip gradients to a maximum norm.\n",
       "- Add a docstring explaining why this is important."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def clip_gradients(grads, max_norm):\n",
       "    \"\"\"\n",
       "    Clip gradients to a maximum norm to prevent exploding gradients.\n",
       "    Used in both RNN and transformer training for stability.\n",
       "    Args:\n",
       "        grads (dict): Dictionary of gradients (arrays).\n",
       "        max_norm (float): Maximum allowed norm.\n",
       "    Returns:\n",
       "        dict: Clipped gradients.\n",
       "    \"\"\"\n",
       "    # TODO: Implement gradient clipping\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ Training Loop with BPTT\n",
       "\n",
       "Combine forward and backward passes to train the RNN on sequence data using BPTT.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is the sequence-level training loop, analogous to how LLMs are trained on long text sequences.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for the RNN training loop using BPTT and gradient clipping.\n",
       "- Add a docstring explaining each step."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def train_rnn_bptt(X_seq, targets_seq, params, loss_fn, lr, epochs, max_norm):\n",
       "    \"\"\"\n",
       "    Train an RNN on sequence data using BPTT and gradient clipping.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        targets_seq (np.ndarray): Target sequence (seq_len,)\n",
       "        params (dict): RNN parameters (W_xh, W_hh, b_h, W_hy, b_y)\n",
       "        loss_fn (callable): Loss function (e.g., cross-entropy)\n",
       "        lr (float): Learning rate.\n",
       "        epochs (int): Number of training epochs.\n",
       "        max_norm (float): Max norm for gradient clipping.\n",
       "    Returns:\n",
       "        dict: Trained parameters.\n",
       "    \"\"\"\n",
       "    # TODO: Implement the RNN training loop with BPTT and gradient clipping\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: BPTT and Sequence Gradients in LLMs\n",
       "\n",
       "- BPTT enables RNNs to learn dependencies across time steps, a key challenge in sequence modeling.\n",
       "- Transformers use attention to address some of the limitations of BPTT, but the core idea of propagating gradients through sequences remains.\n",
       "- Gradient clipping is essential for stable training in both RNNs and transformers.\n",
       "\n",
       "In the next notebook, you'll use these ideas to build a character-level language model!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   