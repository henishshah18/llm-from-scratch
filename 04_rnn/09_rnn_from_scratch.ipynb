{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 09 - RNN from Scratch\n",
       "\n",
       "Recurrent Neural Networks (RNNs) are foundational for sequence modeling. While transformers have largely replaced RNNs in LLMs, understanding RNNs builds intuition for how models process sequences and maintain memory over time.\n",
       "\n",
       "In this notebook, you'll scaffold the core logic of an RNN, step by step, and see how these ideas connect to modern LLMs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üî¢ RNN Cell: The Core Computation\n",
       "\n",
       "An RNN cell processes one time step of a sequence, updating its hidden state:\n",
       "\n",
       "$$ h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h) $$\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- RNNs were the first models to handle sequential data, a key challenge in language modeling. Transformers generalize this idea with self-attention.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for a single RNN cell step.\n",
       "- Add a docstring explaining its role in sequence modeling."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_cell_step(x_t, h_prev, W_xh, W_hh, b_h):\n",
       "    \"\"\"\n",
       "    Compute one step of an RNN cell.\n",
       "    Args:\n",
       "        x_t (np.ndarray): Input at time t (input_dim,)\n",
       "        h_prev (np.ndarray): Previous hidden state (hidden_dim,)\n",
       "        W_xh (np.ndarray): Input-to-hidden weights (hidden_dim x input_dim)\n",
       "        W_hh (np.ndarray): Hidden-to-hidden weights (hidden_dim x hidden_dim)\n",
       "        b_h (np.ndarray): Hidden bias (hidden_dim,)\n",
       "    Returns:\n",
       "        np.ndarray: Current hidden state (hidden_dim,)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the RNN cell computation\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîó Unrolling the RNN Over a Sequence\n",
       "\n",
       "To process a sequence, the RNN cell is applied at each time step, passing the hidden state forward.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is analogous to how transformers process sequences, but with attention instead of recurrence.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run an RNN over an entire input sequence.\n",
       "- Add a docstring explaining the sequence processing."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_forward(X_seq, h0, W_xh, W_hh, b_h):\n",
       "    \"\"\"\n",
       "    Run an RNN over an input sequence.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        W_xh, W_hh, b_h: RNN parameters\n",
       "    Returns:\n",
       "        list: List of hidden states for each time step\n",
       "    \"\"\"\n",
       "    # TODO: Implement RNN unrolling over the sequence\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Output Layer: From Hidden State to Prediction\n",
       "\n",
       "After processing the sequence, the RNN's hidden state(s) are mapped to output predictions (e.g., next token probabilities).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- In language modeling, the output layer predicts the next token at each position, just like in transformers.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute output logits from hidden states.\n",
       "- Add a docstring explaining its use in sequence prediction."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_output_layer(h_states, W_hy, b_y):\n",
       "    \"\"\"\n",
       "    Compute output logits from RNN hidden states.\n",
       "    Args:\n",
       "        h_states (list or np.ndarray): Hidden states (seq_len x hidden_dim)\n",
       "        W_hy (np.ndarray): Hidden-to-output weights (output_dim x hidden_dim)\n",
       "        b_y (np.ndarray): Output bias (output_dim,)\n",
       "    Returns:\n",
       "        np.ndarray: Output logits (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Map hidden states to output logits\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ Training an RNN: Loss and Backpropagation\n",
       "\n",
       "To train an RNN, compute the loss (e.g., cross-entropy) at each time step and backpropagate through time (BPTT).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- BPTT is the precursor to the attention-based backpropagation in transformers.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute the total loss over a sequence (e.g., sum of cross-entropy losses).\n",
       "- Add a docstring explaining its role in sequence modeling."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def rnn_sequence_loss(logits_seq, targets_seq, loss_fn):\n",
       "    \"\"\"\n",
       "    Compute the total loss over a sequence.\n",
       "    Args:\n",
       "        logits_seq (np.ndarray): Output logits (seq_len x output_dim)\n",
       "        targets_seq (np.ndarray): True target indices (seq_len,)\n",
       "        loss_fn (callable): Loss function (e.g., cross-entropy)\n",
       "    Returns:\n",
       "        float: Total loss over the sequence\n",
       "    \"\"\"\n",
       "    # TODO: Compute total sequence loss\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: RNNs and LLMs\n",
       "\n",
       "- RNNs were the first models to handle sequential data, a key challenge in language modeling.\n",
       "- Transformers build on these ideas, replacing recurrence with self-attention for better parallelism and long-range memory.\n",
       "- Understanding RNNs gives you a strong foundation for grasping the inner workings of LLMs and transformers.\n",
       "\n",
       "In the next notebook, you'll dive deeper into backpropagation through time (BPTT) and see how gradients flow through sequences!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   