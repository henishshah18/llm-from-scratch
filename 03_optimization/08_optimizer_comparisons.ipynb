{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6db218e",
   "metadata": {},
   "source": [
    "# 08 - Optimizer Comparisons: SGD, Momentum, RMSProp, Adam\n",
    "\n",
    "Modern LLMs and transformers rely on advanced optimizers to train efficiently and stably. In this notebook, you'll scaffold the core logic of several popular optimizers, and compare their behavior and relevance to LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554091d",
   "metadata": {},
   "source": [
    "## üßÆ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD is the foundation of most optimization algorithms. It updates parameters using the gradient of the loss.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- All advanced optimizers build on the basic idea of SGD.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for a single SGD update step.\n",
    "- Add a docstring explaining its role in LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(params, grads, lr):\n",
    "    \"\"\"\n",
    "    Perform a single SGD update step.\n",
    "    In LLMs, this is the basic building block for parameter updates.\n",
    "    Args:\n",
    "        params (np.ndarray): Current parameter values.\n",
    "        grads (np.ndarray): Gradients of the loss w.r.t. parameters.\n",
    "        lr (float): Learning rate.\n",
    "    Returns:\n",
    "        np.ndarray: Updated parameters.\n",
    "    \"\"\"\n",
    "    # TODO: Implement SGD update\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54a05f",
   "metadata": {},
   "source": [
    "## üèÉ SGD with Momentum\n",
    "\n",
    "Momentum helps accelerate SGD by accumulating a velocity vector in the direction of persistent reduction in the loss.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Momentum is often used to help models converge faster and escape local minima.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for a single SGD with momentum update step.\n",
    "- Add a docstring explaining the velocity term and its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea29df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_step(params, grads, velocity, lr, beta):\n",
    "    \"\"\"\n",
    "    Perform a single SGD with momentum update step.\n",
    "    Momentum helps accelerate convergence in deep networks like LLMs.\n",
    "    Args:\n",
    "        params (np.ndarray): Current parameter values.\n",
    "        grads (np.ndarray): Gradients of the loss w.r.t. parameters.\n",
    "        velocity (np.ndarray): Current velocity vector.\n",
    "        lr (float): Learning rate.\n",
    "        beta (float): Momentum coefficient (0 < beta < 1).\n",
    "    Returns:\n",
    "        tuple: (updated_params, updated_velocity)\n",
    "    \"\"\"\n",
    "    # TODO: Implement momentum update\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89bd6f",
   "metadata": {},
   "source": [
    "## üìâ RMSProp\n",
    "\n",
    "RMSProp adapts the learning rate for each parameter by dividing by a running average of recent gradient magnitudes.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- RMSProp helps stabilize training by normalizing updates, especially in deep or recurrent networks.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for a single RMSProp update step.\n",
    "- Add a docstring explaining the running average and its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_step(params, grads, cache, lr, beta, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Perform a single RMSProp update step.\n",
    "    RMSProp normalizes updates using a running average of squared gradients.\n",
    "    Args:\n",
    "        params (np.ndarray): Current parameter values.\n",
    "        grads (np.ndarray): Gradients of the loss w.r.t. parameters.\n",
    "        cache (np.ndarray): Running average of squared gradients.\n",
    "        lr (float): Learning rate.\n",
    "        beta (float): Decay rate for the running average.\n",
    "        epsilon (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        tuple: (updated_params, updated_cache)\n",
    "    \"\"\"\n",
    "    # TODO: Implement RMSProp update\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590dea1",
   "metadata": {},
   "source": [
    "## ü§ñ Adam Optimizer\n",
    "\n",
    "Adam combines momentum and RMSProp, maintaining both a running average of gradients and squared gradients.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Adam is the most widely used optimizer for training LLMs and transformers due to its stability and efficiency.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function for a single Adam update step.\n",
    "- Add a docstring explaining the moving averages and bias correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d735b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_step(params, grads, m, v, lr, beta1, beta2, t, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Perform a single Adam optimizer update step.\n",
    "    Adam is the default optimizer for most LLMs and transformers.\n",
    "    Args:\n",
    "        params (np.ndarray): Current parameter values.\n",
    "        grads (np.ndarray): Gradients of the loss w.r.t. parameters.\n",
    "        m (np.ndarray): First moment vector (mean of gradients).\n",
    "        v (np.ndarray): Second moment vector (mean of squared gradients).\n",
    "        lr (float): Learning rate.\n",
    "        beta1 (float): Decay rate for first moment.\n",
    "        beta2 (float): Decay rate for second moment.\n",
    "        t (int): Time step (for bias correction).\n",
    "        epsilon (float): Small value to avoid division by zero.\n",
    "    Returns:\n",
    "        tuple: (updated_params, updated_m, updated_v)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Adam update\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d49495",
   "metadata": {},
   "source": [
    "## üìä Comparing Optimizer Performance\n",
    "\n",
    "Different optimizers can converge at different speeds and stabilities.\n",
    "\n",
    "**LLM/Transformer Context:**\n",
    "- Choosing the right optimizer and hyperparameters is crucial for training large models efficiently.\n",
    "\n",
    "### Task:\n",
    "- Scaffold a function to compare the convergence of different optimizers on the same problem.\n",
    "- Add a docstring explaining how this relates to LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimizers(opt_steps, initial_params, compute_loss_and_grads, epochs):\n",
    "    \"\"\"\n",
    "    Compare the convergence of different optimizers on the same loss function.\n",
    "    In LLMs, optimizer choice can affect training speed and final performance.\n",
    "    Args:\n",
    "        opt_steps (dict): Mapping from optimizer name to optimizer step function.\n",
    "        initial_params (np.ndarray): Initial parameter values.\n",
    "        compute_loss_and_grads (callable): Function returning (loss, grads) for current params.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        dict: Mapping from optimizer name to list of loss values per epoch.\n",
    "    \"\"\"\n",
    "    # TODO: Run each optimizer and record loss over epochs\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7051b",
   "metadata": {},
   "source": [
    "## üß† Final Summary: Optimizers in LLMs\n",
    "\n",
    "- Advanced optimizers like Adam are essential for training deep, large-scale models like LLMs and transformers.\n",
    "- Understanding their differences helps you tune and debug training for best results.\n",
    "- In the next notebook, you'll move on to recurrent neural networks (RNNs) and see how optimization applies to sequence modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
