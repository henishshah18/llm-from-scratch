{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 13 - GRU from Scratch\n",
       "\n",
       "Gated Recurrent Units (GRUs) are a simplified variant of LSTMs that also address the vanishing gradient problem in sequence modeling. GRUs are used in some language models and offer a more efficient alternative to LSTMs.\n",
       "\n",
       "In this notebook, you'll scaffold the core logic of a GRU cell and see how it compares to LSTMs and transformers."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ GRU Cell: Gates and State\n",
       "\n",
       "A GRU cell uses two gates to control information flow:\n",
       "- Update gate\n",
       "- Reset gate\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- GRUs, like LSTMs, were designed to model long-range dependencies in sequences. Transformers generalize these ideas with self-attention.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for a single GRU cell step (forward pass).\n",
       "- Add a docstring explaining the role of each gate and state."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def gru_cell_step(x_t, h_prev, params):\n",
       "    \"\"\"\n",
       "    Compute one step of a GRU cell.\n",
       "    Args:\n",
       "        x_t (np.ndarray): Input at time t (input_dim,)\n",
       "        h_prev (np.ndarray): Previous hidden state (hidden_dim,)\n",
       "        params (dict): GRU parameters (weights and biases for all gates)\n",
       "    Returns:\n",
       "        np.ndarray: Current hidden state (hidden_dim,)\n",
       "    \"\"\"\n",
       "    # TODO: Implement the GRU cell computation (gates, candidate state, hidden state)\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Unrolling the GRU Over a Sequence\n",
       "\n",
       "To process a sequence, the GRU cell is applied at each time step, passing the hidden state forward.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- GRUs can model longer dependencies than vanilla RNNs, but transformers go even further with self-attention.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run a GRU over an entire input sequence.\n",
       "- Add a docstring explaining the sequence processing."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def gru_forward(X_seq, h0, params):\n",
       "    \"\"\"\n",
       "    Run a GRU over an input sequence.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        params (dict): GRU parameters\n",
       "    Returns:\n",
       "        list: List of hidden states for each time step\n",
       "    \"\"\"\n",
       "    # TODO: Implement GRU unrolling over the sequence\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Output Layer: From Hidden State to Prediction\n",
       "\n",
       "After processing the sequence, the GRU's hidden state(s) are mapped to output predictions (e.g., next token probabilities).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is analogous to the output projection in transformers for next-token prediction.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute output logits from GRU hidden states.\n",
       "- Add a docstring explaining its use in sequence prediction."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def gru_output_layer(h_states, W_hy, b_y):\n",
       "    \"\"\"\n",
       "    Compute output logits from GRU hidden states.\n",
       "    Args:\n",
       "        h_states (list or np.ndarray): Hidden states (seq_len x hidden_dim)\n",
       "        W_hy (np.ndarray): Hidden-to-output weights (output_dim x hidden_dim)\n",
       "        b_y (np.ndarray): Output bias (output_dim,)\n",
       "    Returns:\n",
       "        np.ndarray: Output logits (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Map hidden states to output logits\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: GRUs and LLMs\n",
       "\n",
       "- GRUs are a simpler, efficient alternative to LSTMs for sequence modeling.\n",
       "- Transformers build on these ideas, using self-attention to model even longer and more flexible dependencies.\n",
       "- Understanding GRUs gives you a strong foundation for appreciating the design of LLMs and transformers.\n",
       "\n",
       "In the next notebook, you'll compare RNNs, LSTMs, and GRUs side by side!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   