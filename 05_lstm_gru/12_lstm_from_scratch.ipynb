{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 12 - LSTM from Scratch\n",
       "\n",
       "Long Short-Term Memory (LSTM) networks are a type of RNN designed to capture long-range dependencies in sequences. LSTMs were a major advance in sequence modeling before transformers, and understanding them helps you appreciate the challenges transformers solve in LLMs.\n",
       "\n",
       "In this notebook, you'll scaffold the core logic of an LSTM cell and see how it improves on basic RNNs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”¢ LSTM Cell: Gates and State\n",
       "\n",
       "An LSTM cell uses gates to control the flow of information and maintain a cell state across time steps:\n",
       "- Forget gate\n",
       "- Input gate\n",
       "- Output gate\n",
       "- Cell state update\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- LSTMs were the state-of-the-art for sequence modeling before transformers. They address the vanishing gradient problem and can model longer dependencies than vanilla RNNs.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function for a single LSTM cell step (forward pass).\n",
       "- Add a docstring explaining the role of each gate and state."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def lstm_cell_step(x_t, h_prev, c_prev, params):\n",
       "    \"\"\"\n",
       "    Compute one step of an LSTM cell.\n",
       "    Args:\n",
       "        x_t (np.ndarray): Input at time t (input_dim,)\n",
       "        h_prev (np.ndarray): Previous hidden state (hidden_dim,)\n",
       "        c_prev (np.ndarray): Previous cell state (hidden_dim,)\n",
       "        params (dict): LSTM parameters (weights and biases for all gates)\n",
       "    Returns:\n",
       "        tuple: (h_t, c_t) - current hidden and cell state\n",
       "    \"\"\"\n",
       "    # TODO: Implement the LSTM cell computation (gates, cell state, hidden state)\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ”— Unrolling the LSTM Over a Sequence\n",
       "\n",
       "To process a sequence, the LSTM cell is applied at each time step, passing both the hidden and cell state forward.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- LSTMs can model longer dependencies than RNNs, but transformers go even further with self-attention.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to run an LSTM over an entire input sequence.\n",
       "- Add a docstring explaining the sequence processing."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def lstm_forward(X_seq, h0, c0, params):\n",
       "    \"\"\"\n",
       "    Run an LSTM over an input sequence.\n",
       "    Args:\n",
       "        X_seq (np.ndarray): Input sequence (seq_len x input_dim)\n",
       "        h0 (np.ndarray): Initial hidden state (hidden_dim,)\n",
       "        c0 (np.ndarray): Initial cell state (hidden_dim,)\n",
       "        params (dict): LSTM parameters\n",
       "    Returns:\n",
       "        list: List of (h_t, c_t) tuples for each time step\n",
       "    \"\"\"\n",
       "    # TODO: Implement LSTM unrolling over the sequence\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§® Output Layer: From Hidden State to Prediction\n",
       "\n",
       "After processing the sequence, the LSTM's hidden state(s) are mapped to output predictions (e.g., next token probabilities).\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This is analogous to the output projection in transformers for next-token prediction.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to compute output logits from LSTM hidden states.\n",
       "- Add a docstring explaining its use in sequence prediction."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def lstm_output_layer(h_states, W_hy, b_y):\n",
       "    \"\"\"\n",
       "    Compute output logits from LSTM hidden states.\n",
       "    Args:\n",
       "        h_states (list or np.ndarray): Hidden states (seq_len x hidden_dim)\n",
       "        W_hy (np.ndarray): Hidden-to-output weights (output_dim x hidden_dim)\n",
       "        b_y (np.ndarray): Output bias (output_dim,)\n",
       "    Returns:\n",
       "        np.ndarray: Output logits (seq_len x output_dim)\n",
       "    \"\"\"\n",
       "    # TODO: Map hidden states to output logits\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## ðŸ§  Final Summary: LSTMs and LLMs\n",
       "\n",
       "- LSTMs were a major advance in sequence modeling, enabling learning of long-range dependencies.\n",
       "- Transformers build on these ideas, using self-attention to model even longer and more flexible dependencies.\n",
       "- Understanding LSTMs gives you a strong foundation for appreciating the design of LLMs and transformers.\n",
       "\n",
       "In the next notebook, you'll explore GRUs, another important RNN variant!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   