{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# 24 - Byte Pair Encoding (BPE) Tokenizer\n",
       "\n",
       "Byte Pair Encoding (BPE) is a subword tokenization algorithm used in many LLMs (e.g., GPT, RoBERTa). It allows models to handle rare words and open vocabularies by splitting words into common subword units.\n",
       "\n",
       "In this notebook, you'll scaffold the steps to build a BPE tokenizer from scratch, as used in modern LLMs."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üìö What is BPE Tokenization?\n",
       "\n",
       "BPE iteratively merges the most frequent pairs of symbols in a corpus, building a vocabulary of subword units.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- BPE enables LLMs to efficiently represent rare and unknown words, improving generalization and reducing vocabulary size.\n",
       "\n",
       "### Task:\n",
       "- Scaffold code to initialize a vocabulary of characters from a text corpus.\n",
       "- Add comments explaining each step."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# TODO: Initialize vocabulary of characters from a text corpus\n",
       "pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîÅ BPE Merge Operations\n",
       "\n",
       "At each step, BPE finds the most frequent pair of symbols and merges them into a new symbol.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- This process builds a vocabulary of subword units that balances frequency and flexibility.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to count symbol pairs in the corpus.\n",
       "- Scaffold a function to merge the most frequent pair.\n",
       "- Add docstrings explaining their roles."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def get_stats(corpus):\n",
       "    \"\"\"\n",
       "    Count frequency of symbol pairs in the corpus.\n",
       "    Args:\n",
       "        corpus (list of list): Corpus as list of tokenized words (list of symbols)\n",
       "    Returns:\n",
       "        dict: Mapping from symbol pair to frequency\n",
       "    \"\"\"\n",
       "    # TODO: Count symbol pairs\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def merge_pair(pair, corpus):\n",
       "    \"\"\"\n",
       "    Merge the most frequent pair in the corpus.\n",
       "    Args:\n",
       "        pair (tuple): Symbol pair to merge\n",
       "        corpus (list of list): Corpus as list of tokenized words\n",
       "    Returns:\n",
       "        list of list: Updated corpus with merged pair\n",
       "    \"\"\"\n",
       "    # TODO: Merge the given pair in the corpus\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üßÆ Building the BPE Vocabulary\n",
       "\n",
       "Repeat the merge operation until the vocabulary reaches the desired size.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- The final BPE vocabulary is used to tokenize text for LLM training and inference.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to build the BPE vocabulary by iteratively merging pairs.\n",
       "- Add a docstring explaining the process."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def build_bpe_vocab(corpus, vocab_size):\n",
       "    \"\"\"\n",
       "    Build a BPE vocabulary by iteratively merging symbol pairs.\n",
       "    Args:\n",
       "        corpus (list of list): Corpus as list of tokenized words\n",
       "        vocab_size (int): Desired vocabulary size\n",
       "    Returns:\n",
       "        set: Final BPE vocabulary\n",
       "    \"\"\"\n",
       "    # TODO: Build BPE vocabulary\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üîó Tokenizing Text with BPE\n",
       "\n",
       "Once the BPE vocabulary is built, tokenize new text by applying the learned merges.\n",
       "\n",
       "**LLM/Transformer Context:**\n",
       "- BPE tokenization is used in LLMs for both training and inference.\n",
       "\n",
       "### Task:\n",
       "- Scaffold a function to tokenize text using the BPE vocabulary.\n",
       "- Add a docstring explaining its use."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def bpe_tokenize(word, bpe_vocab):\n",
       "    \"\"\"\n",
       "    Tokenize a word using the BPE vocabulary.\n",
       "    Args:\n",
       "        word (str): Input word\n",
       "        bpe_vocab (set): Set of BPE tokens\n",
       "    Returns:\n",
       "        list: List of BPE tokens\n",
       "    \"\"\"\n",
       "    # TODO: Tokenize word using BPE merges\n",
       "    pass"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## üß† Final Summary: BPE Tokenization in LLMs\n",
       "\n",
       "- BPE tokenization enables LLMs to efficiently handle rare words and open vocabularies.\n",
       "- Understanding BPE is key to building and interpreting modern LLMs.\n",
       "\n",
       "In the next notebook, you'll explore sampling and decoding strategies for language model inference!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": ""
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   